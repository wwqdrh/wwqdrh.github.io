<!DOCTYPE html><html lang="zh-CN"><head><meta name="viewport" content="width=device-width"/><title>容器核心点</title><meta name="robots" content="follow, index"/><meta charSet="UTF-8"/><meta name="description" content="容器使用与主机的区别"/><meta property="og:type"/><meta property="og:title" content="容器核心点"/><meta property="og:description" content="容器使用与主机的区别"/><meta property="og:url" content="https://wwqdrh.github.io/undefined"/><meta name="keywords" content="wwqdrh技术博客"/><meta property="og:locale" content="zh-CN"/><meta property="og:image" content="https://wwqdrh.github.io"/><meta name="twitter:title" content="容器核心点"/><meta name="twitter:description" content="容器使用与主机的区别"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://wwqdrh.github.io"/><link rel="stylesheet" href="/assets/mdrender/editor-render.css"/><meta name="next-head-count" content="17"/><link rel="icon" href="/favicon.ico"/><link rel="apple-touch-icon" sizes="192x192" href="/apple-touch-icon.png"/><link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/feed"/><script src="/assets/analizy/clarity.js"></script><link rel="preload" href="/_next/static/media/d83e92f0af8b17e4-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/ed42d1b51efd45f6-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/fba9d678ff638e59-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/e52907b750a6f61e-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/9031250013752d4b-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/eb9adf802b0a60eb-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/81b352a4d7a000ae-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/41b9b3ece820718f-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/media/d587d1c112526568-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/869c7de7c781f904.css" as="style"/><link rel="stylesheet" href="/_next/static/css/869c7de7c781f904.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-6ef43a8d4a395f49.js" defer=""></script><script src="/_next/static/chunks/framework-fbe37f60a09a330b.js" defer=""></script><script src="/_next/static/chunks/main-082d90b1269d95f6.js" defer=""></script><script src="/_next/static/chunks/pages/_app-494ee9c4b0594b31.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5B...slug%5D-222bf60d0a2bf3df.js" defer=""></script><script src="/_next/static/PMB5qMppIrri5HN2Gb314/_buildManifest.js" defer=""></script><script src="/_next/static/PMB5qMppIrri5HN2Gb314/_ssgManifest.js" defer=""></script></head><body><div id="__next"><style data-emotion="css-global c86wz">body{margin:0;padding:0;color:hsl(0, 0%, 9.0%);background-color:hsl(0, 0%, 97.3%);font-family:'__pretendard_6bb8e5','__pretendard_Fallback_6bb8e5';}*{-webkit-print-color-scheme:light;color-scheme:light;box-sizing:border-box;}h1,h2,h3,h4,h5,h6{margin:0;font-weight:inherit;font-style:inherit;}a{all:unset;cursor:pointer;}ul{padding:0;}button{all:unset;cursor:pointer;}input{all:unset;box-sizing:border-box;}textarea{border:none;background-color:transparent;font-family:inherit;padding:0;outline:none;resize:none;color:inherit;}hr{width:100%;border:none;margin:0;border-top:1px solid hsl(0, 0%, 88.7%);}</style><style data-emotion="css 1q70a33">.css-1q70a33{z-index:30;position:-webkit-sticky;position:sticky;top:0;background-color:hsl(0, 0%, 97.3%);box-shadow:0 1px 2px 0 rgba(0, 0, 0, 0.05);}.css-1q70a33 .container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:3rem;margin:0 auto;}@media (min-width: 768px){.css-1q70a33 .container[data-full-width="true"]{padding-left:6rem;padding-right:6rem;}}.css-1q70a33 .container .nav{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;gap:0.75rem;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-1q70a33 .container .mid{padding-top:0.5rem;padding-bottom:0.5rem;padding-left:1.25rem;padding-right:1.25rem;border-radius:1rem;outline-style:none;width:50%;background-color:hsl(0, 0%, 93.0%);}</style><div class="py-2 css-1q70a33"><div data-full-width="false" class="container"><a aria-label="wwqdrh" class="css-0" href="/">wwqdrh</a><input class="mid" type="text" placeholder="Search Keyword..." value=""/><div class="nav"><style data-emotion="css 1nw6zn9">.css-1nw6zn9{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;}.css-1nw6zn9 ul{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}.css-1nw6zn9 ul li{display:block;margin-left:1rem;color:hsl(0, 0%, 43.5%);}</style><div class="css-1nw6zn9"><ul><li><a target="_blank" href="https://space.bilibili.com/538676331">B站</a></li></ul></div><style data-emotion="css 1h5x3dy">.css-1h5x3dy{cursor:pointer;}</style><div class="css-1h5x3dy"><style data-emotion="css p95608">.css-p95608{font-family:'__Noto_Color_Emoji_be1378','__Noto_Color_Emoji_Fallback_be1378',Apple Color Emoji;font-weight:400;font-style:normal;}</style><span class="css-p95608">☀️</span></div></div></div></div><style data-emotion="css lomkhl">.css-lomkhl{margin:0 auto;width:100%;padding:0 3rem;}</style><main class="css-lomkhl"><div class="divide-y bg-white dark:bg-gray-700 p-6 shadow-lg rounded-lg mt-3 divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0"><div class="max-w-none pt-10 pb-8 prose dark:prose-dark"><div><img src="/images/blogs/container.png" />

<h1>底层原理</h1>
<p>基于Linux内核的Cgroup、Namespace、UnionFS等技术，对进程进行封装隔离(本质上还是一个进程)，用于操作系统层面的虚拟化技术</p>
<h2>Namespace</h2>
<blockquote>
<p>参考资料
<a href="https://en.wikipedia.org/wiki/Linux_namespaces">https://en.wikipedia.org/wiki/Linux_namespaces</a>
<a href="https://man7.org/linux/man-pages/man7/namespaces.7.html">https://man7.org/linux/man-pages/man7/namespaces.7.html</a></p>
</blockquote>
<p>对namespace的操作方法</p>
<p><code>clone</code></p>
<p>在创建新进程的系统调用时，通过CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET等创建新的命名空间</p>
<p><code>setns</code></p>
<p>让一个调用进程加入某个已经存在的Namespace中</p>
<p><code>unshare</code></p>
<p>将调用进程移动到新的Namespace下, 各个namespace下的资源配置则是通过各种容器管理工具(docker、kubernetes)，例如通过<code>unshare -fn [进程命令]</code>,创建的network namespace就是只有loopback</p>
<p>用用命令</p>
<ul>
<li>查看当前系统所在命名空间: <code>lsns -t [类型]</code></li>
<li>查看某进程的namespace: <code>ls -la /proc/&lt;pid&gt;/ns/</code></li>
<li>进入某namespace运行命令: <code>nsenter -t &lt;pid&gt; -n ip addr</code></li>
</ul>
<blockquote>
<p>提供共不同类型资源的隔离</p>
</blockquote>
<p>内核级别的系统资源隔离方式</p>
<p>主机有其默认的Namespace，行为与用户的Namespace 完全一致，不同进程可以通过不同的Namespace 进行隔离。</p>
<p>当系统启动一个容器时，会为该容器创建相应的不同类型的Namespace，为运行在容器内的进程提供相应的资源隔离。</p>
<p>每个Namespace都附带一个编号，是该Namespace在系统中的唯一标识。</p>
<p>对Namespace操作相关的系统调用</p>
<ul>
<li>clone：创建新进程的系统调用时，通过flags参数指定需要新建的Namespace类型</li>
<li>setns：让调用进程加入某个已经存在的Namespace</li>
<li>unshare：将调用进程移动到新的Namespace下</li>
</ul>
<h3>Mount (mnt)</h3>
<p>挂载点</p>
<ul>
<li><code>/proc/[pid]/ns/mnt</code></li>
</ul>
<blockquote>
<p>kernel 2.4.19</p>
</blockquote>
<p>挂载点</p>
<p>在主机上<code>/proc/[pid]/mounts</code>，<code>/proc/[pid]/mountinfo</code>(可以看到该pid所属的mount Namespace下的挂载点的传播类型及所属的对等组)，<code>/proc/[pid]/mountstats</code>等文件查看挂载点。</p>
<p>在容器内，通过mount或lsmnt命令查看Mount Namespace的有效挂载点。</p>
<p>挂载点传播类型</p>
<ul>
<li>MS_SHARED</li>
<li>MS_PRIVATE</li>
<li>MS_SLAVE</li>
<li>MS_UNBINDABLE</li>
</ul>
<h3>Process ID (pid)</h3>
<p>进程id</p>
<ul>
<li><code>/proc/[pid]/ns/pid</code></li>
<li><code>/proc/[pid]/ns/pid_for_children</code></li>
</ul>
<blockquote>
<p>kernel 2.6.14</p>
</blockquote>
<p>进程，在主机的<code>/proc/[pid]/ns</code>目录中可以查看系统进程归属的Namespace。</p>
<p>容器启动后，Entrypoint 进程会作为PID 为1 的进程存在，因此是该PID Namespace 的init 进程。</p>
<p>它是当前Namespace所有进程的父进程，如果该进程退出，内核会对该PID Namespace的所有进程发送SIGKILL 信号，以便同时结束它们。</p>
<p>init 进程默认屏蔽系统信号，即除非该进程对系统信号做特殊处理，否则发往该进程的系统信号默认都会被忽略。不过SIGKILL 和SIGSTOP 信号比较特殊，init 进程无法捕获这两个信号。</p>
<p>Kubernetes 默认对同一Pod 的不同容器构建独立的PIDNamespace，以便将不同容器的进程彼此隔离，同时允许通过ShareProcessNamespace 属性设置不同容器的进程共享PID Namespace。</p>
<p>Kubernetes 支持多重容器进程的重启策略，默认行为是用户进程退出后立即重启。Kubernetes 用户只需中止其容器中的Entrypoint 进程，即可实现容器重启。</p>
<h3>Network (net)</h3>
<ul>
<li><code>/proc/[pid]/ns/net</code></li>
</ul>
<blockquote>
<p>kernel 2.6.29</p>
</blockquote>
<p>一个物理的网络设备通常会放到系统初始的Network Namespace中。</p>
<ul>
<li>网络设备</li>
<li>网络协议栈</li>
<li>网络端口</li>
<li>IP路由表</li>
<li>防火墙规则</li>
<li><code>/proc/net</code></li>
<li><code>/sys/class/net</code></li>
<li><code>/proc/sys/net</code></li>
</ul>
<p>不同网络的Namespace 由网络虚拟设备（Virtual EthernetDevice，即VETH）连通，再基于网桥或者路由实现与物理网络设备的连通。当网络Namespace 被释放后，对应的VETH Pair 设备也会被自动释放。</p>
<p>在Kubernetes 中，同一Pod 的不同容器共享同一网络的Namespace，没有例外。这使得Kubernetes 能将网络挂载在更轻量、更稳定的sandbox 容器上，而用户定义的容器只需复用已配置好的网络即可。另外，同一Pod 的不同容器中运行的进程可以基于localhost彼此通信</p>
<h3>Interprocess Communication (ipc)</h3>
<ul>
<li><code>/proc/[pid]/ns/ipc</code></li>
</ul>
<blockquote>
<p>kernel 2.6.19</p>
</blockquote>
<p>System V IPC和POSIX消息队列（都是进程间通信相关的资源）</p>
<p>System V IPC 对象包含信号量、共享内存和消息队列，用于进程间的通信。System V IPC对象具有全局唯一的标识，对在该IPC Namespace 内的进程可见，而对其外的进程不可见。当IPC Namespace 被销毁后，所有的IPC 对象也会被自动销毁。</p>
<p>Kubernetes 允许用户在Pod 中使用hostIPC 进行定义，通过该属性使授权用户容器共享主机IPC Namespace，达到进程间通信的目的。</p>
<h3>UTS</h3>
<ul>
<li><code>/proc/[pid]/ns/uts</code></li>
</ul>
<blockquote>
<p>kernel 2.6.19</p>
</blockquote>
<p>主机名和域名，UTS Namespace中的一个进程可以看做一个在网络上独立存在的节点（除IP外，还能通过主机名进行访问）</p>
<h3>User ID (user)</h3>
<p>用户用户组</p>
<ul>
<li><code>/proc/[pid]/ns/user</code></li>
</ul>
<blockquote>
<p>kernel 3.8</p>
</blockquote>
<blockquote>
<p>权限隔离和用户身份标识隔离，在容器类创建切换用户，以及为文件目录设置不同的用户权限，从而实现容器内的权限管理，而无须影响主机配置。</p>
</blockquote>
<p>主要隔离安全相关的标识符和属性，用户ID、用户组ID、root 目录、秘钥等。</p>
<p>一个进程的用户ID 和组ID 在User Namespace 内外可以有所不同。在该User Namespace 外，它是一个非特权的用户ID；而在User Namespace 内，进程可以使用0（root）作为用户ID，且其具有完全的特权权限。</p>
<h3>Control group (cgroup) Namespace</h3>
<ul>
<li><code>/proc/[pid]/ns/cgroup</code></li>
</ul>
<h3>Time Namespace</h3>
<ul>
<li><code>/proc/[pid]/ns/time</code></li>
<li><code>/proc/[pid]/ns/time_for_children</code></li>
</ul>
<h2>cgroup</h2>
<p>Linux下用于对一个或一组进程进行资源控制和监控的机制，可以对诸如CPU使用时间、内存、磁盘I/O等进程所需的资源进行限制。</p>
<p>不同资源的具体管理工作由对应的Cgroup子系统来实现，以层级树(Hierarchy)的方式来组织管理，每个Cgroup都可以包含其他的子Group，因此子Cgroup能使用的资源除了受本Cgroup配置的资源参数限制，还受到父CGroup设置的资源限制，针对不同类型的资源限制，只要讲限制策略在不同的子系统上进行关联即可</p>
<blockquote>
<p>kernel 4.6</p>
</blockquote>
<p>内核从 4.6 版本开始支持 CGroup Namespace。如果容器启动时没有开启 CGroup Namespace，那么在容器内部查询 CGroup 时，返回整个系统的信息；而开启 CGroup Namespace 后，可以看到当前容器以根形式展示的单独的CGroup 信息。</p>
<h3>cgroup v1</h3>
<img src="/images/blogs/groupv1.jpeg" />


<p>v1 的CPU Cgroup，memory Cgroup和blkio Cgroup，那么Cgroup v1的一个整体结构，你应该已经很熟悉了。它的每一个子系统都是独立的，资源的限制只能在子系统中发生。</p>
<h3>cgroup v2</h3>
<img src="/images/blogs/groupv2.jpeg" />

<p>Cgroup v2相比Cgroup v1做的最大的变动就是一个进程属于一个控制组，而每个控制组里可以定义自己需要的多个子系统。</p>
<p>比如Cgroup V2中，某个进程pid_y属于控制组group2，而在group2里同时打开了io和memory子系统 （Cgroup V2里的io子系统就等同于Cgroup v1里的blkio子系统）。</p>
<p>那么，Cgroup对进程pid_y的磁盘 I/O做限制的时候，就可以考虑到进程pid_y写入到Page Cache内存的页面了，这样buffered I/O的磁盘限速就实现了。</p>
<p>目前即使最新版本的Ubuntu Linux或者Centos Linux，仍然在使用Cgroup v1作为缺省的Cgroup。打开方法就是配置一个kernel参数&quot;cgroup_no_v1=blkio,memory&quot;，这表示把Cgroup v1的blkio和Memory两个子系统给禁止，我们可以把这个参数配置到grub中，然后我们重启Linux机器，这时Cgroup v2的 io还有Memory这两个子系统，它们的功能就打开了。</p>
<h3>docker中对cgroup的应用</h3>
<img src="/images/blogs/cgroup-docker.png" />

<h3>CPU</h3>
<ul>
<li>cpu.shares</li>
<li>cpu.cfs_period_us和cpu.cfs_quota_us</li>
<li>cpu.stat:</li>
<li>nr_periods</li>
<li>nr_throttled</li>
</ul>
<h3>cpuacct</h3>
<p>统计CGroup及其子CGroup下进程的CPPU使用情况</p>
<ul>
<li>cpuacct.usage</li>
<li>cpuacct.stat</li>
</ul>
<h3>cpuset</h3>
<p>分配单独的CPU和内存节点，将进程固定在某个CPU或内存节点上，以达到提高性能的目的</p>
<ul>
<li>cpuset.cpus</li>
<li>cpuset.mems</li>
<li>cpuset.memory_migrate</li>
<li>cpuset.cpu_exclusive</li>
<li>cpuset.mem_exclusive</li>
</ul>
<h3>memory</h3>
<p>用于限制CGroup下进程的内存使用量</p>
<ul>
<li>memory.stat</li>
<li>memory.usage_in_bytes</li>
<li>memory.max_usage_in_bytes</li>
<li>memory.limit_in_bytes</li>
<li>memory.failcnt</li>
<li>memory.force_empty</li>
<li>memory.oom_control</li>
</ul>
<h3>blkio</h3>
<p>对块设备放的IO控制，按权重分配目前有两种限制方式：一是限制每秒写入的字节数（Bytes Per Second，即BPS），二是限制每秒的读写次数（I/O Per Second，即IOPS）</p>
<p>依赖于磁盘的CFQ调度，如果磁盘调度使用deadline或者none的算法则无法支持</p>
<p><strong>权重方式的配置</strong></p>
<ul>
<li>blkio.weight</li>
<li>blkio.weight_device</li>
</ul>
<p><strong>IO限流方式的配置</strong></p>
<p>![[Pasted image 20220524111700.png]]</p>
<h3>PID</h3>
<p>PID 子系统用来限制CGroup 能够创建的进程数。</p>
<p>● pids.max：允许创建的最大进程数量。
● pids.current：当前的进程数量。</p>
<h3>其他</h3>
<p>● devices 子系统，控制进程访问某些设备
● perf_event 子系统，控制perf 监控CGroup 下的进程。
● net_cls 子系统，标记CGroups 中进程的网络数据包，通过TC 模块（Traffic Control）对数据包进行控制。
● net_prio 子系统，针对每个网络设备设置特定的优先级。
● hugetlb 子系统，对hugepage 的使用进行限制。
● freezer 子系统，挂起或者恢复CGroups 中的进程。
● ns 子系统，使不同CGroups 下面的进程使用不同的Namespace。
● rdma 子系统，对RDMA/IB-spe-cific 资源进行限制</p>
<h1>容器进程</h1>
<h2>namespace</h2>
<h3>pids</h3>
<img src="/images/blogs/namespace-pid.jpeg" />

<h2>cgroup</h2>
<h3>pids</h3>
<p>pids Cgroup 通过 Cgroup 文件系统的方式向用户提供操作接口，一般它的 Cgroup 文件系统挂载点在 /sys/fs/cgroup/pids。</p>
<p>在一个容器建立之后，创建容器的服务会在 /sys/fs/cgroup/pids 下建立一个子目录，就是一个控制组，控制组里最关键的一个文件就是 pids.max。我们可以向这个文件写入数值，而这个值就是这个容器中允许的最大进程数目。</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># pwd</span>
/sys/fs/cgroup/pids
<span class="hljs-comment"># df ./</span>
Filesystem     1K-blocks  Used Available Use% Mounted on
cgroup                 <span class="hljs-number">0</span>     <span class="hljs-number">0</span>         <span class="hljs-number">0</span>    - /sys/fs/cgroup/pids
<span class="hljs-comment"># docker ps</span>
CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS               NAMES
7ecd3aa7fdc1        registry/zombie-proc:v1   <span class="hljs-string">&quot;/app-test 1000&quot;</span>         <span class="hljs-number">37</span> hours ago        Up <span class="hljs-number">37</span> hours                             frosty_yalow

<span class="hljs-comment"># pwd</span>
/sys/fs/cgroup/pids/system.<span class="hljs-built_in">slice</span>/docker-7ecd3aa7fdc15a1e183813b1899d5d939beafb11833ad6c8b0432536e5b9871c.scope

<span class="hljs-comment"># ls</span>
cgroup.clone_children  cgroup.procs  notify_on_release  pids.current  pids.events  pids.<span class="hljs-built_in">max</span>  tasks
<span class="hljs-comment"># echo 1002 &gt; pids.max</span>
<span class="hljs-comment"># cat pids.max</span>
<span class="hljs-number">1002</span>
</code></pre><h3>CPU</h3>
<blockquote>
<p>这个cgroup的使用方式就是创建好对应的文件，然后把程序的pid加入进去</p>
<p>./threads-cpu/threads-cpu 2 &amp;
echo $! &gt; /sys/fs/cgroup/cpu/group2/group3/cgroup.procs</p>
</blockquote>
<p>第一个参数是 cpu.cfs_period_us，它是 CFS 算法的一个调度周期，一般它的值是 100000，以 microseconds 为单位，也就 100ms。</p>
<p>第二个参数是 cpu.cfs_quota_us，它“表示 CFS 算法中，在一个调度周期里这个控制组被允许的运行时间，比如这个值为 50000 时，就是 50ms。</p>
<p>如果用这个值去除以调度周期（也就是 cpu.cfs_period_us），50ms/100ms = 0.5，这样这个控制组被允许使用的 CPU 最大配额就是 0.5 个 CPU。</p>
<p>从这里能够看出，cpu.cfs_quota_us 是一个绝对值。如果这个值是 200000，也就是 200ms，那么它除以 period，也就是 200ms/100ms=2。</p>
<p>第三个参数， cpu.shares。这个值是 CPU Cgroup 对于控制组之间的 CPU 分配比例，它的缺省值是 1024。不同组之间会按照这个值的比例进行分配，例如(group3 中的 cpu.shares 是 1024，而 group4 中的 cpu.shares 是 3072，那么 group3:group4=1:3, 在一台 4 个 CPU 的机器上，当 group3 和 group4 都需要 4 个 CPU 的时候，它们实际分配到的 CPU 分别是这样的：group3 是 1 个，group4 是 3 个。)</p>
<h2>init进程</h2>
<p>容器init进程对SIGKILL和SIGTERM信号的处理</p>
<p>Linux 内核针对每个 Nnamespace 里的 init 进程，把只有 default handler 的信号都给忽略了。</p>
<p>如果我们自己注册了信号的 handler（应用程序注册信号 handler 被称作&quot;Catch the Signal&quot;），那么这个信号 handler 就不再是 SIG_DFL 。即使是 init 进程在接收到 SIGTERM 之后也是可以退出的。</p>
<p>不过，由于 SIGKILL 是一个特例，因为 SIGKILL 是不允许被注册用户 handler 的（还有一个不允许注册用户 handler 的信号是 SIGSTOP），那么它只有 SIG_DFL handler。</p>
<p>所以 init 进程是永远不能被 SIGKILL 所杀，但是可以被 SIGTERM 杀死。</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># ## golang init</span>
<span class="hljs-comment"># cat /proc/1/status | grep -i SigCgt</span>
<span class="hljs-attr">SigCgt:</span>     <span class="hljs-string">fffffffe7fc1feff</span>

<span class="hljs-comment"># ## C init</span>
<span class="hljs-comment"># cat /proc/1/status | grep -i SigCgt</span>
<span class="hljs-attr">SigCgt:</span>     <span class="hljs-number">0000000000000000</span>

<span class="hljs-comment"># ## bash init</span>
<span class="hljs-comment"># cat /proc/1/status | grep -i SigCgt</span>
<span class="hljs-attr">SigCgt:</span>     <span class="hljs-number">0000000000010002</span>
</code></pre><p>内核为什么必须要给容器中的子进程发 SIGKILL 信号，而不是直接发送 SIGTERM 信号？这样不就不用转发了吗？</p>
<p>因为 SIGTERM 信号可以被捕获，如果容器内有进程忽略该信号，那么关闭容器后，就有进程残留或僵尸进程。从这点来说，使用 SIGKILL 进行强杀，也是无奈之举。</p>
<p>容器中一个标准的 init 进程应该具备哪些能力？</p>
<ul>
<li>（1）至少可以转发 SIGTERM 信号给容器里的其他进程。</li>
<li>（2）能够接收外部的 SIGTERM 信号而退出（在 tini 中，init 进程将所有 SIGTERM 信号转发给子进程后，后面还有回收僵尸进程的流程，在这里 init 进程发现所有子进程都退出了，就会让自己也退出）。</li>
<li>（3）具备清理僵尸进程的能力。</li>
</ul>
<h2>如何拿到正确的CPU开销</h2>
<p>在容器中运行 top 命令，虽然可以看到容器中每个进程的 CPU 使用率，但是 top 中&quot;%Cpu(s)&quot;那一行中显示的数值，并不是这个容器的 CPU 整体使用率，而是容器宿主机的 CPU 使用率。</p>
<p>对于每个进程，top 都会从 proc 文件系统中每个进程对应的 stat 文件<code>/proc/[pid]/stat</code>中读取 2 个数值utime(进程的用户态部分在 Linux 调度中获得 CPU 的 ticks)和stime(进程的内核态部分在 Linux 调度中获得 CPU 的 ticks)</p>
<p>由于 /proc/stat 文件是整个节点全局的状态文件，不属于任何一个 Namespace，因此在容器中无法通过读取 /proc/stat 文件来获取单个容器的 CPU 使用率。</p>
<p>所以要得到单个容器的 CPU 使用率，我们可以从 CPU Cgroup 每个控制组里的统计文件 cpuacct.stat 中获取。单个容器 CPU 使用率 =((utime_2 – utime_1) + (stime_2 – stime_1)) * 100.0 / (HZ * et * 1 )。</p>
<h1>容器内存</h1>
<h2>namespace</h2>
<h2>cgroup</h2>
<h3>memory</h3>
<p>Memory Cgroup 的虚拟文件系统的挂载点一般在&quot;/sys/fs/cgroup/memory&quot;这个目录下,可以在 Memory Cgroup 的挂载点目录下，创建一个子目录作为控制组。</p>
<ul>
<li>memory.limit_in_bytes: 一个控制组里所有进程可使用内存的最大值</li>
<li>memory.oom_control: 当控制组中的进程内存使用达到上限值时，这个参数能够决定会不会触发 OOM Killer(不希望触发 OOM Killer，只要执行 echo 1 &gt; memory.oom_control)</li>
<li>memory.usage_in_bytes: 只读的，它里面的数值是当前控制组里所有进程实际使用的内存总和。</li>
<li>memory.swappiness: 用于控制是否使用swap分区</li>
</ul>
<p>对于每个容器创建后，系统都会为它建立一个 Memory Cgroup 的控制组，容器的所有进程都在这个控制组里。</p>
<p>容器里所有进程使用的内存量，超过了容器所在 Memory Cgroup 里的内存限制。这时 Linux 系统就会主动杀死容器中的一个进程，往往这会导致整个容器的退出。</p>
<p>为什么 memory.usage_in_bytes 与 memory.limit_in_bytes 的值只相差了 90KB，我们在容器中还是可以申请出 50MB 的物理内存？</p>
<p>Memory Cgroup 控制组里 RSS 内存和 Page Cache 内存的和，正好是 memory.usage_in_bytes 的值。</p>
<p>当控制组里的进程需要申请新的物理内存，而且 memory.usage_in_bytes 里的值超过控制组里的内存上限值 memory.limit_in_bytes，这时我们前面说的 Linux 的内存回收（page frame reclaim）就会被调用起来。</p>
<p>那么在这个控制组里的 page cache 的内存会根据新申请的内存大小释放一部分，这样我们还是能成功申请到新的物理内存，整个控制组里总的物理内存开销 memory.usage_in_bytes 还是不会超过上限值 memory.limit_in_bytes。</p>
<pre><code class="hljs language-bash">docker run -d --name mem_alloc registry/<span class="hljs-attr">mem_alloc</span>:v1

sleep <span class="hljs-number">2</span>
<span class="hljs-variable constant_">CONTAINER_ID</span>=$(sudo docker ps --format <span class="hljs-string">&quot;\{\{.ID\}\}\t\{\{.Names\}\}&quot;</span> | grep -i mem_alloc | awk <span class="hljs-string">&#x27;{print $1}&#x27;</span>)
echo $CONTAINER_ID

<span class="hljs-variable constant_">CGROUP_CONTAINER_PATH</span>=$(find /sys/fs/cgroup/memory/ -name <span class="hljs-string">&quot;*$CONTAINER_ID*&quot;</span>)
echo $CGROUP_CONTAINER_PATH

echo <span class="hljs-number">536870912</span> &gt; $CGROUP_CONTAINER_PATH/memory.<span class="hljs-property">limit_in_bytes</span>
cat $CGROUP_CONTAINER_PATH/memory.<span class="hljs-property">limit_in_bytes</span>
</code></pre><p>memory.limit_in_bytes: 设置容器最大的内存使用量</p>
<blockquote>
<p>如果我们运行docker inspect 命令查看容器退出的原因，就会看到容器处于&quot;exited&quot;状态，并且&quot;OOMKilled&quot;是 true。</p>
</blockquote>
<p>在发生 OOM 的时候,Linux根据进程已经使用的物理内存页面数，每个进程的OOM校准值oom_score_adj(在 /proc 文件系统中，每个进程都有一个 /proc//oom_score_adj 的接口文件。我们可以在这个文件中输入 -1000 到 1000 之间的任意一个数值，调整进程被 OOM Kill 的几率。)来决定杀死哪个进程的，用系统总的可用页面数，去乘以 OOM 校准值 oom_score_adj，再加上进程已经使用的物理页面数，计算出来的值越大，那么这个进程被 OOM Kill 的几率也就越大。</p>
<h2>swap分区</h2>
<p>memory.swappiness 可以控制这个 Memroy Cgroup 控制组下面匿名内存和 page cache 的回收，取值的范围和工作方式和全局的 swappiness 差不多。这里有一个优先顺序，在 Memory Cgorup 的控制组里，如果你设置了 memory.swappiness 参数，它就会覆盖全局的 swappiness，让全局的 swappiness 在这个控制组里不起作用。</p>
<p>不过有一点和全局的swappiness不同的是：当 memory.swappiness = 0 的时候，对匿名页的回收是始终禁止的，也就是始终都不会使用 Swap 空间。</p>
<p>这时 Linux 系统不会再去比较 free 内存和 zone 里的 high water mark 的值，再决定一个 Memory Cgroup 中的匿名内存要不要回收了。</p>
<h1>容器存储</h1>
<h2>namespace</h2>
<h3>mount</h3>
<p>容器中的根文件系统，其实就是我们做的镜像。</p>
<p>Mount Namespace 保证了每个容器都有自己独立的文件目录结构。</p>
<p>可以通过 /proc/mounts 这个路径，找到容器 OverlayFS 对应的 lowerdir 和 upperdir。</p>
<h2>cgroups</h2>
<h3>quota</h3>
<p>从宿主机的角度看，upperdir 就是一个目录，如果容器不断往容器文件系统中写入数据，实际上就是往宿主机的磁盘上写数据，这些数据也就存在于宿主机的磁盘目录中。</p>
<p>当然对于容器来说，如果有大量的写操作是不建议写入容器文件系统的，一般是需要给容器挂载一个 volume，用来满足大量的文件读写。</p>
<p>但是不能避免的是，用户在容器中运行的程序有错误，或者进行了错误的配置。</p>
<p>比如说，我们把 log 写在了容器文件系统上，并且没有做 log rotation，那么时间一久，就会导致宿主机上的磁盘被写满。这样影响的就不止是容器本身了，而是整个宿主机了</p>
<p>因为 upperdir 在宿主机上也是一个普通的目录, 可以通过限制 upperdir 目录容量的方式，来限制一个容器 OverlayFS 根目录的写入数据量,对于 Linux 上最常用的两个文件系统 XFS 和 ext4，它们有一个特性 Quota,这个特性可以为 Linux 系统里的一个用户（user），一个用户组（group）或者一个项目（project）来限制它们使用文件系统的额度（quota），也就是限制它们可以写入文件系统的文件总量。</p>
<p>要使用 XFS Quota 特性，必须在文件系统挂载的时候加上对应的 Quota 选项，比如我们目前需要配置 Project Quota，那么这个挂载参数就是&quot;pquota&quot;。对于根目录来说，这个参数必须作为一个内核启动的参数&quot;rootflags=pquota&quot;，这样设置就可以保证根目录在启动挂载的时候，带上 XFS Quota 的特性并且支持 Project 模式。我们可以从 /proc/mounts 信息里，看看根目录是不是带&quot;prjquota&quot;字段。如果里面有这个字段，就可以确保文件系统已经带上了支持 project 模式的 XFS quota 特性。</p>
<p>通过 xfs_quota 这条命令,给一个指定的目录打上一个 Project ID, 未这个id做Quota限制</p>
<p><code>xfs_quota -x -c &#39;limit -p bhard=10m 101&#39; /</code></p>
<p>在用 docker run 启动容器的时候，加上一个参数 --storage-opt size= ，就能限制住容器 OverlayFS 文件系统可写入的最大数据量了。</p>
<h3>blkio</h3>
<blockquote>
<p>Direct I/O可以通过blkio Cgroup来限制磁盘I/O，但是Buffered I/O不能被限制。这个问题只有在cgroupv2版本中才解决了</p>
</blockquote>
<p>不过容器文件系统并不适合频繁地读写。对于频繁读写的数据，容器需要把他们到放到&quot;volume&quot;中。这里的volume可以是一个本地的磁盘，也可以是一个网络磁盘。如果多个容器同时读写是会产生影响的，我们可以限制某一个的读写速率</p>
<p>Cgroup v1中有blkio子系统, 可以来限制磁盘的I/O。不过blkio子系统对于磁盘I/O的限制，并不像CPU，Memory那么直接</p>
<p>在Cgroups v1里，blkio Cgroup的虚拟文件系统挂载点一般在&quot;/sys/fs/cgroup/blkio/&quot;。</p>
<ul>
<li>blkio.throttle.read_iops_device</li>
<li>blkio.throttle.read_bps_device</li>
<li>blkio.throttle.write_iops_device</li>
<li>blkio.throttle.write_bps_device</li>
</ul>
<pre><code class="hljs language-bash"><span class="hljs-comment"># &quot;252:16&quot;是要写入的设备例如/dev/vdb的主次设备号，你可以通过 ls -l /dev/vdb 看到这两个值</span>
<span class="hljs-comment"># 设置写入吞吐量不超过10MB/s</span>
echo <span class="hljs-string">&quot;252:16 10485760&quot;</span> &gt; $CGROUP_CONTAINER_PATH/blkio.throttle.write_bps_device
</code></pre><h3>io</h3>
<blockquote>
<p>cgroupv2 版本中的系统，用于解决无法对buffered io限速的问题</p>
</blockquote>
<pre><code class="hljs language-bash"><span class="hljs-comment"># Create a new control group</span>
mkdir -p /sys/fs/cgroup/unified/iotest

<span class="hljs-comment"># enable the io and memory controller subsystem</span>
echo <span class="hljs-string">&quot;+io +memory&quot;</span> &gt; /sys/fs/cgroup/unified/cgroup.subtree_control

<span class="hljs-comment"># Add current bash pid in iotest control group.</span>
<span class="hljs-comment"># Then all child processes of the bash will be in iotest group too,</span>
<span class="hljs-comment"># including the fio</span>

echo $$ &gt;/sys/fs/cgroup/unified/iotest/cgroup.procs

<span class="hljs-comment"># 256:16 are device major and minor ids, /mnt is on the device.</span>
echo <span class="hljs-string">&quot;252:16 wbps=10485760&quot;</span> &gt; /sys/fs/cgroup/unified/iotest/io.<span class="hljs-built_in">max</span>
cd /mnt
<span class="hljs-comment">#Run the fio in non direct I/O mode</span>
fio -iodepth=<span class="hljs-number">1</span> -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=<span class="hljs-number">1</span>  -name=./fio.test
</code></pre><h2>容器镜像的文件系统</h2>
<p>在容器里，运行 df 命令，你可以看到在容器中根目录 (/) 的文件系统类型是&quot;overlay&quot;，它不是我们在普通 Linux 节点上看到的 Ext4 或者 XFS 之类常见的文件系统。</p>
<p>如果没有特别的容器文件系统，只是普通的 Ext4 或者 XFS 文件系统，那么每次启动一个容器，就需要把一个镜像文件下载并且存储在宿主机上。overlay文件系统就是用来解决这种冗余的</p>
<blockquote>
<p>在 Linux 内核 3.18 版本中，OverlayFS 代码正式合入 Linux 内核的主分支。在这之后，OverlayFS 也就逐渐成为各个主流 Linux 发行版本里缺省使用的容器文件系统了。</p>
</blockquote>
<p>overlay在分类中属于UnionFS，UnionFS 这类文件系统实现的主要功能是把多个目录（处于不同的分区）一起挂载（mount）在一个目录下。</p>
<img src="/images/blogs/unionfs示意图.jpeg" />

<blockquote>
<p>&quot;work/&quot;，一个存放临时文件的目录，OverlayFS 中如果有文件修改，就会在中间过程中临时存放文件到这里。</p>
</blockquote>
<pre><code class="hljs language-bash"><span class="hljs-comment"># !/bin/bash</span>

umount ./merged
rm upper lower merged work -r

mkdir upper lower merged work
echo <span class="hljs-string">&quot;I&#x27;m from lower!&quot;</span> &gt; lower/in_lower.txt
echo <span class="hljs-string">&quot;I&#x27;m from upper!&quot;</span> &gt; upper/in_upper.txt
<span class="hljs-comment"># `in_both` is in both directories</span>
echo <span class="hljs-string">&quot;I&#x27;m from lower!&quot;</span> &gt; lower/in_both.txt
echo <span class="hljs-string">&quot;I&#x27;m from upper!&quot;</span> &gt; upper/in_both.txt

sudo mount -t overlay overlay \
 -o lowerdir=./lower,upperdir=./upper,workdir=./work \
 ./merged
</code></pre><p>OverlayFS 也是把多个目录合并挂载，被挂载的目录分为两大类：lowerdir 和 upperdir。</p>
<p>lowerdir 允许有多个目录，在被挂载后，这些目录里的文件都是不会被修改或者删除的，也就是只读的；upperdir 只有一个，不过这个目录是可读写的，挂载点目录中的所有文件修改都会在 upperdir 中反映出来。</p>
<p>在merged文件夹中进行操作的效果:</p>
<ul>
<li>新建文件，这个文件会出现在 upper/ 目录中。</li>
<li>删除文件，如果我们删除&quot;in_upper.txt&quot;，那么这个文件会在 upper/ 目录中消失。如果删除&quot;in_lower.txt&quot;, 在 lower/ 目录里的&quot;in_lower.txt&quot;文件不会有变化，只是在 upper/ 目录中增加了一个特殊文件来告诉 OverlayFS，&quot;in_lower.txt&#39;这个文件不能出现在 merged/ 里了，这就表示它已经被删除了。</li>
<li>修改文件, 类似如果修改&quot;in_lower.txt&quot;，那么就会在 upper/ 目录中新建一个&quot;in_lower.txt&quot;文件，包含更新的内容，而在 lower/ 中的原来的实际文件&quot;in_lower.txt&quot;不会改变。</li>
</ul>
<h2>磁盘读写</h2>
<p>Blkio Cgroup: 在容器中对磁盘I/O的限速</p>
<p>Linux内存和文件系统管理对容器IO的影响: Page Frame Reclaim, Dirty Page</p>
<h1>容器网络</h1>
<h2>namespace</h2>
<h3>network</h3>
<p>隔离的资源包括</p>
<ul>
<li>第一种，网络设备，lo，eth0等</li>
<li>第二种是 IPv4 和 IPv6 协议栈。IP 层以及上面的 TCP 和 UDP 协议栈也是每个 Namespace 独立工作的。所以 IP、TCP、UDP 的很多协议，它们的相关参数也是每个 Namespace 独立的，这些参数大多数都在 /proc/sys/net/ 目录下面，同时也包括了 TCP 和 UDP 的 port 资源。</li>
<li>第三种，IP 路由表</li>
<li>第四种是iptables 规则了</li>
<li>第五种是网络的状态信息，这些信息你可以从 /proc/net 和 /sys/class/net 里得到</li>
</ul>
<p>tcp_keepalive 的三个参数都是重新初始化的，而 tcp_congestion_control 的值是从 Host Namespace 里复制过来的。</p>
<p>在启动普通容器，尝试一下在容器里去修改&quot;/proc/sys/net/&quot;下的参数，会报错</p>
<blockquote>
<p>只读模式下，容器中&quot;/proc/sys/&quot;是只读 mount 的，那么在容器里是不能修改&quot;/proc/sys/net/&quot;下面的任何参数了。</p>
</blockquote>
<p>想修改 Network Namespace 里的网络参数，要选择容器刚刚启动，而容器中的应用程序还没启动之前进行。runC 也在对 /proc/sys 目录做 read-only mount 之前，预留出了修改接口，就是用来修改容器里 &quot;/proc/sys&quot;下参数的，同样也是 sysctl 的参数。而 Docker 的–sysctl或者 Kubernetes 里的allowed-unsafe-sysctls特性也都利用了 runC 的 sysctl 参数修改接口，允许容器在启动时修改容器 Namespace 里的参数。</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># docker run -d --name net_para --sysctl net.ipv4.tcp_keepalive_time=600 centos:8.1.1911 sleep 3600</span>
7efed88a44d64400ff5a6d38fdcc73f2a74a7bdc3dbc7161060f2f7d0be170d1
<span class="hljs-comment"># docker exec net_para cat /proc/sys/net/ipv4/tcp_keepalive_time</span>
<span class="hljs-number">600</span>
</code></pre><img src="/images/blogs/networknamespace工具包.jpeg" />

<h2>cgroup</h2>
<h2>容器网络接口</h2>
<img src="/images/blogs/network网络模式.jpeg" />

<p>第一步，就是要让数据包从容器的 Network Namespace 发送到 Host Network Namespace 上。</p>
<p>一般来说就只有两类设备接口：一类是veth，另外一类是 macvlan/ipvlan。</p>
<p>用 Docker 启动的容器缺省的网络接口用的也是这个 veth</p>
<img src="/images/blogs/veth.jpeg" />

<pre><code class="hljs language-bash"><span class="hljs-comment"># 示例 创建一个无网络的容器，手动创建一个veth网卡</span>
docker run -d --name <span class="hljs-keyword">if</span>-test --network none centos:<span class="hljs-number">8.1</span><span class="hljs-number">.1911</span> sleep <span class="hljs-number">36000</span>

pid=$(ps -ef | grep <span class="hljs-string">&quot;sleep 36000&quot;</span> | grep -v grep | awk <span class="hljs-string">&#x27;{print $2}&#x27;</span>)
echo $pid

<span class="hljs-comment"># 在&quot;/var/run/netns/&quot;的目录下建立一个符号链接，指向这个容器的 Network Namespace。完成这步操作之后，在后面的&quot;ip netns&quot;操作里，就可以用 pid 的值作为这个容器的 Network Namesapce 的标识了。</span>
ln -s /proc/$pid/ns/net /var/run/netns/$pid

<span class="hljs-comment"># 建立一对 veth 的虚拟设备接口</span>
ip link add name veth_host <span class="hljs-built_in">type</span> veth peer name veth_container
<span class="hljs-comment"># 把 veth_container 这个接口放入到容器的 Network Namespace 中</span>
ip link <span class="hljs-built_in">set</span> veth_container netns $pid

<span class="hljs-comment"># In the container, setup veth_container</span>
ip netns <span class="hljs-built_in">exec</span> $pid ip link <span class="hljs-built_in">set</span> veth_container name eth0
ip netns <span class="hljs-built_in">exec</span> $pid ip addr add <span class="hljs-number">172.17</span><span class="hljs-number">.1</span><span class="hljs-number">.2</span>/<span class="hljs-number">16</span> dev eth0
ip netns <span class="hljs-built_in">exec</span> $pid ip link <span class="hljs-built_in">set</span> eth0 up
ip netns <span class="hljs-built_in">exec</span> $pid ip route add default via <span class="hljs-number">172.17</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span>

<span class="hljs-comment"># In the host, set veth_host up</span>
ip link <span class="hljs-built_in">set</span> veth_host up
</code></pre><p>ipvlan网络配置方式</p>
<blockquote>
<p>对于延时敏感的应用程序，我们可以考虑使用 ipvlan/macvlan 网络接口的容器。不过，由于 ipvlan/macvlan 网络接口直接挂载在物理网络接口上，对于需要使用 iptables 规则的容器，比如 Kubernetes 里使用 service 的容器，就不能工作了。</p>
</blockquote>
<p>无论是 macvlan 还是 ipvlan，它们都是在一个物理的网络接口上再配置几个虚拟的网络接口。在这些虚拟的网络接口上，都可以配置独立的 IP，并且这些 IP 可以属于不同的 Namespace。对于 macvlan，每个虚拟网络接口都有自己独立的 mac 地址；而 ipvlan 的虚拟网络接口是和物理网络接口共享同一个 mac 地址。而且它们都有自己的 L2/L3 的配置方式</p>
<img src="/images/blogs/ipvlan.jpeg" />

<pre><code class="hljs language-bash">docker run --init --name lat-test-<span class="hljs-number">1</span> --network none -d registry/latency-test:v1 sleep <span class="hljs-number">36000</span>

pid1=$(docker inspect lat-test-<span class="hljs-number">1</span> | grep -i Pid | head -n <span class="hljs-number">1</span> | awk <span class="hljs-string">&#x27;{print $2}&#x27;</span> | awk -F <span class="hljs-string">&quot;,&quot;</span> <span class="hljs-string">&#x27;{print $1}&#x27;</span>)
echo $pid1
ln -s /proc/$pid1/ns/net /var/run/netns/$pid1
 
ip link add link eth0 ipvt1 <span class="hljs-built_in">type</span> ipvlan mode l2
ip link <span class="hljs-built_in">set</span> dev ipvt1 netns $pid1

ip netns <span class="hljs-built_in">exec</span> $pid1 ip link <span class="hljs-built_in">set</span> ipvt1 name eth0
ip netns <span class="hljs-built_in">exec</span> $pid1 ip addr add <span class="hljs-number">172.17</span><span class="hljs-number">.3</span><span class="hljs-number">.2</span>/<span class="hljs-number">16</span> dev eth0
ip netns <span class="hljs-built_in">exec</span> $pid1 ip link <span class="hljs-built_in">set</span> eth0 up
</code></pre><p>第二步，数据包发到了 Host Network Namespace 之后，还要解决数据包怎么从宿主机上的 eth0 发送出去的问题。</p>
<p>这一步呢，就是一个普通 Linux 节点上数据包转发的问题了。用 nat 来做个转发，或者建立 Overlay 网络发送，也可以通过配置 proxy arp 加路由的方法来实现。</p>
<p>Docker 缺省使用的是 bridge + nat 的转发方式</p>
<p>Docker 程序在节点上安装完之后，就会自动建立了一个 docker0 的 bridge interface。所以我们只需要把第一步中建立的 veth_host 这个设备，接入到 docker0 这个 bridge 上。</p>
<img src="/images/blogs/网络第二步.png" />

<p>容器和 docker0 组成了一个子网，docker0 上的 IP 就是这个子网的网关 IP。如果我们要让子网通过宿主机上 eth0 去访问外网的话，那么加上 iptables 的规则就可以了<code>iptables -P FORWARD ACCEPT</code></p>
<p>两个网络设备接口之间的数据包转发，需要ip_forward参数，所以还需要把这个改为1</p>
<pre><code class="hljs language-bash">echo <span class="hljs-number">1</span> &gt; <span class="hljs-regexp">/proc/</span>sys/net/ipv4/ip_forward
</code></pre><h2>容器网络的问题</h2>
<p>延时增加, 因为多了虚拟网卡的转发以及各种路由</p>
<p>乱序包的增加</p>
<p>在云平台的这种网络环境里，网络包乱序 +SACK 之后，产生的数据包重传的量要远远高于网络丢包引起的重传。</p>
<p>把 RPS 的这个特性配置到 veth 网络接口上，来减少数据包乱序的几率。不过RPS 的配置还是会带来额外的系统开销，在某些网络环境中会引起 softirq CPU 使用率的增大</p>
<p>尽管容器中 root 用户的 Linux capabilities 已经减少了很多，但是在没有 User Namespace 的情况下，容器中 root 用户和宿主机上的 root 用户的 uid 是完全相同的，一旦有软件的漏洞，容器中的 root 用户就可以操控整个宿主机。</p>
<p>User Namespace，它带来的好处有两个。一个是把容器中 root 用户（uid 0）映射成宿主机上的普通用户，另外一个好处是在云平台里对于容器 uid 的分配要容易些。</p>
<p>为了减少安全风险，业界都是建议在容器中以非 root 用户来运行进程。不过在没有 User Namespace 的情况下，在容器中使用非 root 用户，对于容器云平台来说，对 uid 的管理会比较麻烦。</p>
<p>除了在容器中以非 root 用户来运行进程外，Docker 和 podman 都支持了 rootless container，也就是说它们都可以以非 root 用户来启动和管理容器，这样就进一步降低了容器的安全风险。</p>
<h1>容器安全</h1>
<p>对于容器的 root 用户，缺省只赋予了 15 个 capabilities。如果我们发现容器中进程的权限不够，就需要分析它需要的最小 capabilities 集合，而不是直接赋予容器&quot;privileged&quot;。</p>
<p>因为&quot;privileged&quot;包含了所有的 Linux capabilities, 这样&quot;privileged&quot;就可以轻易获取宿主机上的所有资源，这会对宿主机的安全产生威胁。容器平台上是基本不允许把容器直接设置为&quot;privileged&quot;的，我们需要根据容器中进程需要的最少特权来赋予 capabilities。</p>
<p>假设容器里需要使用 iptables。因为使用 iptables 命令，只需要设置 CAP_NET_ADMIN 这个 capability 就行。那么我们只要在运行 Docker 的时候，给这个容器再多加一个 NET_ADMIN 参数就可以了。</p>
<pre><code class="hljs language-bash"><span class="hljs-comment"># docker run --name iptables --cap-add NET_ADMIN -it registry/iptables:v1 bash</span>
[root@cfedf124dcf1 /]<span class="hljs-comment"># iptables -L</span>
Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
</code></pre><h2>not a root</h2>
<p>如果不想让容器以 root 用户运行，最直接的办法就是给容器指定一个普通用户 uid</p>
<pre><code class="hljs language-bash"><span class="hljs-string">docker</span> <span class="hljs-string">run</span> <span class="hljs-string">-ti</span> <span class="hljs-string">--name</span> <span class="hljs-string">root_example</span> <span class="hljs-string">-u</span> <span class="hljs-number">6667</span><span class="hljs-string">:6667</span> <span class="hljs-string">-v</span> <span class="hljs-string">/etc:/mnt</span>  <span class="hljs-string">centos</span> <span class="hljs-string">bash</span>
</code></pre><p>还有另外一个办法，就是我们在创建容器镜像的时候，用 Dockerfile 为容器镜像里建立一个用户。这样操作以后，容器里缺省的进程都会以这个用户启动。</p>
<pre><code class="hljs language-dockerfile"><span class="hljs-string">FROM</span> <span class="hljs-string">centos</span>

<span class="hljs-string">RUN</span> <span class="hljs-string">adduser</span> <span class="hljs-string">-u</span> <span class="hljs-number">6667 </span><span class="hljs-string">nonroot</span>
<span class="hljs-string">USER</span> <span class="hljs-string">nonroot</span>
</code></pre><p>但是这种方法会导致当多个客户的容器在同一个节点上运行的时候，其实就都使用了宿主机上 uid 6667。然而Linux 系统上，每个用户下的资源是有限制的，比如打开文件数目（open files）、最大进程数目（max user processes）等等。一旦有很多个容器共享一个 uid，这些容器就很可能很快消耗掉这个 uid 下的资源，这样很容易导致这些容器都不能再正常工作。</p>
<h2>Usernamespace</h2>
<blockquote>
<p>k8s还未引入</p>
</blockquote>
<p>User Namespace 隔离了一台 Linux 节点上的 User ID（uid）和 Group ID（gid），它给 Namespace 中的 uid/gid 的值与宿主机上的 uid/gid 值建立了一个映射关系。经过 User Namespace 的隔离，我们在 Namespace 中看到的进程的 uid/gid，就和宿主机 Namespace 中看到的 uid 和 gid 不一样了。</p>
<blockquote>
<p>跟 Docker 相比，podman 不再有守护进程 dockerd，而是直接通过 fork/execve 的方式来启动一个新的容器。这种方式启动容器更加简单，也更容易维护。</p>
</blockquote>
<p><code>podman run -ti  -v /etc:/mnt --uidmap 0:2000:1000 centos bash</code></p>
<p><code>0:2000:1000</code>意思是第一个 0 是指在新的 Namespace 里 uid 从 0 开始，中间的那个 2000 指的是 Host Namespace 里被映射的 uid 从 2000 开始，最后一个 1000 是指总共需要连续映射 1000 个 uid。</p>
<p>容器里的uid 0 是被映射到宿主机上的 uid 2000</p>
<h2>rootless container</h2>
<p>rootless container 中的&quot;rootless&quot;不仅仅指容器中以非 root 用户来运行进程，还指以非 root 用户来创建容器，管理容器。也就是说，启动容器的时候，Docker 或者 podman 是以非 root 用户来执行的。</p>
<pre><code class="hljs language-bash">$ <span class="hljs-built_in">id</span>
uid=<span class="hljs-number">1001</span>(redhat) gid=<span class="hljs-number">1001</span>(redhat) groups=<span class="hljs-number">1001</span>(redhat)
$ podman run -it  ubi7/ubi bash   <span class="hljs-comment">### 在宿主机上以redhat用户启动容器</span>
[root@206f6d5cb033 /]<span class="hljs-comment"># id     ### 容器中的用户是root</span>
uid=<span class="hljs-number">0</span>(root) gid=<span class="hljs-number">0</span>(root) groups=<span class="hljs-number">0</span>(root)
[root@206f6d5cb033 /]<span class="hljs-comment"># sleep 3600   ### 在容器中启动一个sleep 进程</span>

<span class="hljs-comment"># ps -ef |grep sleep   ###在宿主机上查看容器sleep进程对应的用户</span>
redhat   <span class="hljs-number">29433</span> <span class="hljs-number">29410</span>  <span class="hljs-number">0</span> 05:<span class="hljs-number">14</span> pts/<span class="hljs-number">0</span>    <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span> sleep <span class="hljs-number">3600</span>
</code></pre></div></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"mdxSource":"\u003cimg src=\"/images/blogs/container.png\" /\u003e\n\n\u003ch1\u003e底层原理\u003c/h1\u003e\n\u003cp\u003e基于Linux内核的Cgroup、Namespace、UnionFS等技术，对进程进行封装隔离(本质上还是一个进程)，用于操作系统层面的虚拟化技术\u003c/p\u003e\n\u003ch2\u003eNamespace\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e参考资料\n\u003ca href=\"https://en.wikipedia.org/wiki/Linux_namespaces\"\u003ehttps://en.wikipedia.org/wiki/Linux_namespaces\u003c/a\u003e\n\u003ca href=\"https://man7.org/linux/man-pages/man7/namespaces.7.html\"\u003ehttps://man7.org/linux/man-pages/man7/namespaces.7.html\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e对namespace的操作方法\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eclone\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在创建新进程的系统调用时，通过CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET等创建新的命名空间\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esetns\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e让一个调用进程加入某个已经存在的Namespace中\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eunshare\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e将调用进程移动到新的Namespace下, 各个namespace下的资源配置则是通过各种容器管理工具(docker、kubernetes)，例如通过\u003ccode\u003eunshare -fn [进程命令]\u003c/code\u003e,创建的network namespace就是只有loopback\u003c/p\u003e\n\u003cp\u003e用用命令\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e查看当前系统所在命名空间: \u003ccode\u003elsns -t [类型]\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e查看某进程的namespace: \u003ccode\u003els -la /proc/\u0026lt;pid\u0026gt;/ns/\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e进入某namespace运行命令: \u003ccode\u003ensenter -t \u0026lt;pid\u0026gt; -n ip addr\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e提供共不同类型资源的隔离\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e内核级别的系统资源隔离方式\u003c/p\u003e\n\u003cp\u003e主机有其默认的Namespace，行为与用户的Namespace 完全一致，不同进程可以通过不同的Namespace 进行隔离。\u003c/p\u003e\n\u003cp\u003e当系统启动一个容器时，会为该容器创建相应的不同类型的Namespace，为运行在容器内的进程提供相应的资源隔离。\u003c/p\u003e\n\u003cp\u003e每个Namespace都附带一个编号，是该Namespace在系统中的唯一标识。\u003c/p\u003e\n\u003cp\u003e对Namespace操作相关的系统调用\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eclone：创建新进程的系统调用时，通过flags参数指定需要新建的Namespace类型\u003c/li\u003e\n\u003cli\u003esetns：让调用进程加入某个已经存在的Namespace\u003c/li\u003e\n\u003cli\u003eunshare：将调用进程移动到新的Namespace下\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eMount (mnt)\u003c/h3\u003e\n\u003cp\u003e挂载点\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/mnt\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003ekernel 2.4.19\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e挂载点\u003c/p\u003e\n\u003cp\u003e在主机上\u003ccode\u003e/proc/[pid]/mounts\u003c/code\u003e，\u003ccode\u003e/proc/[pid]/mountinfo\u003c/code\u003e(可以看到该pid所属的mount Namespace下的挂载点的传播类型及所属的对等组)，\u003ccode\u003e/proc/[pid]/mountstats\u003c/code\u003e等文件查看挂载点。\u003c/p\u003e\n\u003cp\u003e在容器内，通过mount或lsmnt命令查看Mount Namespace的有效挂载点。\u003c/p\u003e\n\u003cp\u003e挂载点传播类型\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMS_SHARED\u003c/li\u003e\n\u003cli\u003eMS_PRIVATE\u003c/li\u003e\n\u003cli\u003eMS_SLAVE\u003c/li\u003e\n\u003cli\u003eMS_UNBINDABLE\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eProcess ID (pid)\u003c/h3\u003e\n\u003cp\u003e进程id\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/pid\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/pid_for_children\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003ekernel 2.6.14\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e进程，在主机的\u003ccode\u003e/proc/[pid]/ns\u003c/code\u003e目录中可以查看系统进程归属的Namespace。\u003c/p\u003e\n\u003cp\u003e容器启动后，Entrypoint 进程会作为PID 为1 的进程存在，因此是该PID Namespace 的init 进程。\u003c/p\u003e\n\u003cp\u003e它是当前Namespace所有进程的父进程，如果该进程退出，内核会对该PID Namespace的所有进程发送SIGKILL 信号，以便同时结束它们。\u003c/p\u003e\n\u003cp\u003einit 进程默认屏蔽系统信号，即除非该进程对系统信号做特殊处理，否则发往该进程的系统信号默认都会被忽略。不过SIGKILL 和SIGSTOP 信号比较特殊，init 进程无法捕获这两个信号。\u003c/p\u003e\n\u003cp\u003eKubernetes 默认对同一Pod 的不同容器构建独立的PIDNamespace，以便将不同容器的进程彼此隔离，同时允许通过ShareProcessNamespace 属性设置不同容器的进程共享PID Namespace。\u003c/p\u003e\n\u003cp\u003eKubernetes 支持多重容器进程的重启策略，默认行为是用户进程退出后立即重启。Kubernetes 用户只需中止其容器中的Entrypoint 进程，即可实现容器重启。\u003c/p\u003e\n\u003ch3\u003eNetwork (net)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/net\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003ekernel 2.6.29\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e一个物理的网络设备通常会放到系统初始的Network Namespace中。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e网络设备\u003c/li\u003e\n\u003cli\u003e网络协议栈\u003c/li\u003e\n\u003cli\u003e网络端口\u003c/li\u003e\n\u003cli\u003eIP路由表\u003c/li\u003e\n\u003cli\u003e防火墙规则\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/proc/net\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/sys/class/net\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/proc/sys/net\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e不同网络的Namespace 由网络虚拟设备（Virtual EthernetDevice，即VETH）连通，再基于网桥或者路由实现与物理网络设备的连通。当网络Namespace 被释放后，对应的VETH Pair 设备也会被自动释放。\u003c/p\u003e\n\u003cp\u003e在Kubernetes 中，同一Pod 的不同容器共享同一网络的Namespace，没有例外。这使得Kubernetes 能将网络挂载在更轻量、更稳定的sandbox 容器上，而用户定义的容器只需复用已配置好的网络即可。另外，同一Pod 的不同容器中运行的进程可以基于localhost彼此通信\u003c/p\u003e\n\u003ch3\u003eInterprocess Communication (ipc)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/ipc\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003ekernel 2.6.19\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSystem V IPC和POSIX消息队列（都是进程间通信相关的资源）\u003c/p\u003e\n\u003cp\u003eSystem V IPC 对象包含信号量、共享内存和消息队列，用于进程间的通信。System V IPC对象具有全局唯一的标识，对在该IPC Namespace 内的进程可见，而对其外的进程不可见。当IPC Namespace 被销毁后，所有的IPC 对象也会被自动销毁。\u003c/p\u003e\n\u003cp\u003eKubernetes 允许用户在Pod 中使用hostIPC 进行定义，通过该属性使授权用户容器共享主机IPC Namespace，达到进程间通信的目的。\u003c/p\u003e\n\u003ch3\u003eUTS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/uts\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003ekernel 2.6.19\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e主机名和域名，UTS Namespace中的一个进程可以看做一个在网络上独立存在的节点（除IP外，还能通过主机名进行访问）\u003c/p\u003e\n\u003ch3\u003eUser ID (user)\u003c/h3\u003e\n\u003cp\u003e用户用户组\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/user\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003ekernel 3.8\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e权限隔离和用户身份标识隔离，在容器类创建切换用户，以及为文件目录设置不同的用户权限，从而实现容器内的权限管理，而无须影响主机配置。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e主要隔离安全相关的标识符和属性，用户ID、用户组ID、root 目录、秘钥等。\u003c/p\u003e\n\u003cp\u003e一个进程的用户ID 和组ID 在User Namespace 内外可以有所不同。在该User Namespace 外，它是一个非特权的用户ID；而在User Namespace 内，进程可以使用0（root）作为用户ID，且其具有完全的特权权限。\u003c/p\u003e\n\u003ch3\u003eControl group (cgroup) Namespace\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/cgroup\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eTime Namespace\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/time\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e/proc/[pid]/ns/time_for_children\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003ecgroup\u003c/h2\u003e\n\u003cp\u003eLinux下用于对一个或一组进程进行资源控制和监控的机制，可以对诸如CPU使用时间、内存、磁盘I/O等进程所需的资源进行限制。\u003c/p\u003e\n\u003cp\u003e不同资源的具体管理工作由对应的Cgroup子系统来实现，以层级树(Hierarchy)的方式来组织管理，每个Cgroup都可以包含其他的子Group，因此子Cgroup能使用的资源除了受本Cgroup配置的资源参数限制，还受到父CGroup设置的资源限制，针对不同类型的资源限制，只要讲限制策略在不同的子系统上进行关联即可\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003ekernel 4.6\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e内核从 4.6 版本开始支持 CGroup Namespace。如果容器启动时没有开启 CGroup Namespace，那么在容器内部查询 CGroup 时，返回整个系统的信息；而开启 CGroup Namespace 后，可以看到当前容器以根形式展示的单独的CGroup 信息。\u003c/p\u003e\n\u003ch3\u003ecgroup v1\u003c/h3\u003e\n\u003cimg src=\"/images/blogs/groupv1.jpeg\" /\u003e\n\n\n\u003cp\u003ev1 的CPU Cgroup，memory Cgroup和blkio Cgroup，那么Cgroup v1的一个整体结构，你应该已经很熟悉了。它的每一个子系统都是独立的，资源的限制只能在子系统中发生。\u003c/p\u003e\n\u003ch3\u003ecgroup v2\u003c/h3\u003e\n\u003cimg src=\"/images/blogs/groupv2.jpeg\" /\u003e\n\n\u003cp\u003eCgroup v2相比Cgroup v1做的最大的变动就是一个进程属于一个控制组，而每个控制组里可以定义自己需要的多个子系统。\u003c/p\u003e\n\u003cp\u003e比如Cgroup V2中，某个进程pid_y属于控制组group2，而在group2里同时打开了io和memory子系统 （Cgroup V2里的io子系统就等同于Cgroup v1里的blkio子系统）。\u003c/p\u003e\n\u003cp\u003e那么，Cgroup对进程pid_y的磁盘 I/O做限制的时候，就可以考虑到进程pid_y写入到Page Cache内存的页面了，这样buffered I/O的磁盘限速就实现了。\u003c/p\u003e\n\u003cp\u003e目前即使最新版本的Ubuntu Linux或者Centos Linux，仍然在使用Cgroup v1作为缺省的Cgroup。打开方法就是配置一个kernel参数\u0026quot;cgroup_no_v1=blkio,memory\u0026quot;，这表示把Cgroup v1的blkio和Memory两个子系统给禁止，我们可以把这个参数配置到grub中，然后我们重启Linux机器，这时Cgroup v2的 io还有Memory这两个子系统，它们的功能就打开了。\u003c/p\u003e\n\u003ch3\u003edocker中对cgroup的应用\u003c/h3\u003e\n\u003cimg src=\"/images/blogs/cgroup-docker.png\" /\u003e\n\n\u003ch3\u003eCPU\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecpu.shares\u003c/li\u003e\n\u003cli\u003ecpu.cfs_period_us和cpu.cfs_quota_us\u003c/li\u003e\n\u003cli\u003ecpu.stat:\u003c/li\u003e\n\u003cli\u003enr_periods\u003c/li\u003e\n\u003cli\u003enr_throttled\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003ecpuacct\u003c/h3\u003e\n\u003cp\u003e统计CGroup及其子CGroup下进程的CPPU使用情况\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecpuacct.usage\u003c/li\u003e\n\u003cli\u003ecpuacct.stat\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003ecpuset\u003c/h3\u003e\n\u003cp\u003e分配单独的CPU和内存节点，将进程固定在某个CPU或内存节点上，以达到提高性能的目的\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecpuset.cpus\u003c/li\u003e\n\u003cli\u003ecpuset.mems\u003c/li\u003e\n\u003cli\u003ecpuset.memory_migrate\u003c/li\u003e\n\u003cli\u003ecpuset.cpu_exclusive\u003c/li\u003e\n\u003cli\u003ecpuset.mem_exclusive\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003ememory\u003c/h3\u003e\n\u003cp\u003e用于限制CGroup下进程的内存使用量\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ememory.stat\u003c/li\u003e\n\u003cli\u003ememory.usage_in_bytes\u003c/li\u003e\n\u003cli\u003ememory.max_usage_in_bytes\u003c/li\u003e\n\u003cli\u003ememory.limit_in_bytes\u003c/li\u003e\n\u003cli\u003ememory.failcnt\u003c/li\u003e\n\u003cli\u003ememory.force_empty\u003c/li\u003e\n\u003cli\u003ememory.oom_control\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eblkio\u003c/h3\u003e\n\u003cp\u003e对块设备放的IO控制，按权重分配目前有两种限制方式：一是限制每秒写入的字节数（Bytes Per Second，即BPS），二是限制每秒的读写次数（I/O Per Second，即IOPS）\u003c/p\u003e\n\u003cp\u003e依赖于磁盘的CFQ调度，如果磁盘调度使用deadline或者none的算法则无法支持\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e权重方式的配置\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eblkio.weight\u003c/li\u003e\n\u003cli\u003eblkio.weight_device\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eIO限流方式的配置\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e![[Pasted image 20220524111700.png]]\u003c/p\u003e\n\u003ch3\u003ePID\u003c/h3\u003e\n\u003cp\u003ePID 子系统用来限制CGroup 能够创建的进程数。\u003c/p\u003e\n\u003cp\u003e● pids.max：允许创建的最大进程数量。\n● pids.current：当前的进程数量。\u003c/p\u003e\n\u003ch3\u003e其他\u003c/h3\u003e\n\u003cp\u003e● devices 子系统，控制进程访问某些设备\n● perf_event 子系统，控制perf 监控CGroup 下的进程。\n● net_cls 子系统，标记CGroups 中进程的网络数据包，通过TC 模块（Traffic Control）对数据包进行控制。\n● net_prio 子系统，针对每个网络设备设置特定的优先级。\n● hugetlb 子系统，对hugepage 的使用进行限制。\n● freezer 子系统，挂起或者恢复CGroups 中的进程。\n● ns 子系统，使不同CGroups 下面的进程使用不同的Namespace。\n● rdma 子系统，对RDMA/IB-spe-cific 资源进行限制\u003c/p\u003e\n\u003ch1\u003e容器进程\u003c/h1\u003e\n\u003ch2\u003enamespace\u003c/h2\u003e\n\u003ch3\u003epids\u003c/h3\u003e\n\u003cimg src=\"/images/blogs/namespace-pid.jpeg\" /\u003e\n\n\u003ch2\u003ecgroup\u003c/h2\u003e\n\u003ch3\u003epids\u003c/h3\u003e\n\u003cp\u003epids Cgroup 通过 Cgroup 文件系统的方式向用户提供操作接口，一般它的 Cgroup 文件系统挂载点在 /sys/fs/cgroup/pids。\u003c/p\u003e\n\u003cp\u003e在一个容器建立之后，创建容器的服务会在 /sys/fs/cgroup/pids 下建立一个子目录，就是一个控制组，控制组里最关键的一个文件就是 pids.max。我们可以向这个文件写入数值，而这个值就是这个容器中允许的最大进程数目。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# pwd\u003c/span\u003e\n/sys/fs/cgroup/pids\n\u003cspan class=\"hljs-comment\"\u003e# df ./\u003c/span\u003e\nFilesystem     1K-blocks  Used Available Use% Mounted on\ncgroup                 \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e     \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e         \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e    - /sys/fs/cgroup/pids\n\u003cspan class=\"hljs-comment\"\u003e# docker ps\u003c/span\u003e\nCONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS               NAMES\n7ecd3aa7fdc1        registry/zombie-proc:v1   \u003cspan class=\"hljs-string\"\u003e\u0026quot;/app-test 1000\u0026quot;\u003c/span\u003e         \u003cspan class=\"hljs-number\"\u003e37\u003c/span\u003e hours ago        Up \u003cspan class=\"hljs-number\"\u003e37\u003c/span\u003e hours                             frosty_yalow\n\n\u003cspan class=\"hljs-comment\"\u003e# pwd\u003c/span\u003e\n/sys/fs/cgroup/pids/system.\u003cspan class=\"hljs-built_in\"\u003eslice\u003c/span\u003e/docker-7ecd3aa7fdc15a1e183813b1899d5d939beafb11833ad6c8b0432536e5b9871c.scope\n\n\u003cspan class=\"hljs-comment\"\u003e# ls\u003c/span\u003e\ncgroup.clone_children  cgroup.procs  notify_on_release  pids.current  pids.events  pids.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e  tasks\n\u003cspan class=\"hljs-comment\"\u003e# echo 1002 \u0026gt; pids.max\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# cat pids.max\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e1002\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003eCPU\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e这个cgroup的使用方式就是创建好对应的文件，然后把程序的pid加入进去\u003c/p\u003e\n\u003cp\u003e./threads-cpu/threads-cpu 2 \u0026amp;\necho $! \u0026gt; /sys/fs/cgroup/cpu/group2/group3/cgroup.procs\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e第一个参数是 cpu.cfs_period_us，它是 CFS 算法的一个调度周期，一般它的值是 100000，以 microseconds 为单位，也就 100ms。\u003c/p\u003e\n\u003cp\u003e第二个参数是 cpu.cfs_quota_us，它“表示 CFS 算法中，在一个调度周期里这个控制组被允许的运行时间，比如这个值为 50000 时，就是 50ms。\u003c/p\u003e\n\u003cp\u003e如果用这个值去除以调度周期（也就是 cpu.cfs_period_us），50ms/100ms = 0.5，这样这个控制组被允许使用的 CPU 最大配额就是 0.5 个 CPU。\u003c/p\u003e\n\u003cp\u003e从这里能够看出，cpu.cfs_quota_us 是一个绝对值。如果这个值是 200000，也就是 200ms，那么它除以 period，也就是 200ms/100ms=2。\u003c/p\u003e\n\u003cp\u003e第三个参数， cpu.shares。这个值是 CPU Cgroup 对于控制组之间的 CPU 分配比例，它的缺省值是 1024。不同组之间会按照这个值的比例进行分配，例如(group3 中的 cpu.shares 是 1024，而 group4 中的 cpu.shares 是 3072，那么 group3:group4=1:3, 在一台 4 个 CPU 的机器上，当 group3 和 group4 都需要 4 个 CPU 的时候，它们实际分配到的 CPU 分别是这样的：group3 是 1 个，group4 是 3 个。)\u003c/p\u003e\n\u003ch2\u003einit进程\u003c/h2\u003e\n\u003cp\u003e容器init进程对SIGKILL和SIGTERM信号的处理\u003c/p\u003e\n\u003cp\u003eLinux 内核针对每个 Nnamespace 里的 init 进程，把只有 default handler 的信号都给忽略了。\u003c/p\u003e\n\u003cp\u003e如果我们自己注册了信号的 handler（应用程序注册信号 handler 被称作\u0026quot;Catch the Signal\u0026quot;），那么这个信号 handler 就不再是 SIG_DFL 。即使是 init 进程在接收到 SIGTERM 之后也是可以退出的。\u003c/p\u003e\n\u003cp\u003e不过，由于 SIGKILL 是一个特例，因为 SIGKILL 是不允许被注册用户 handler 的（还有一个不允许注册用户 handler 的信号是 SIGSTOP），那么它只有 SIG_DFL handler。\u003c/p\u003e\n\u003cp\u003e所以 init 进程是永远不能被 SIGKILL 所杀，但是可以被 SIGTERM 杀死。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# ## golang init\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# cat /proc/1/status | grep -i SigCgt\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eSigCgt:\u003c/span\u003e     \u003cspan class=\"hljs-string\"\u003efffffffe7fc1feff\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# ## C init\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# cat /proc/1/status | grep -i SigCgt\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eSigCgt:\u003c/span\u003e     \u003cspan class=\"hljs-number\"\u003e0000000000000000\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# ## bash init\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# cat /proc/1/status | grep -i SigCgt\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eSigCgt:\u003c/span\u003e     \u003cspan class=\"hljs-number\"\u003e0000000000010002\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e内核为什么必须要给容器中的子进程发 SIGKILL 信号，而不是直接发送 SIGTERM 信号？这样不就不用转发了吗？\u003c/p\u003e\n\u003cp\u003e因为 SIGTERM 信号可以被捕获，如果容器内有进程忽略该信号，那么关闭容器后，就有进程残留或僵尸进程。从这点来说，使用 SIGKILL 进行强杀，也是无奈之举。\u003c/p\u003e\n\u003cp\u003e容器中一个标准的 init 进程应该具备哪些能力？\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e（1）至少可以转发 SIGTERM 信号给容器里的其他进程。\u003c/li\u003e\n\u003cli\u003e（2）能够接收外部的 SIGTERM 信号而退出（在 tini 中，init 进程将所有 SIGTERM 信号转发给子进程后，后面还有回收僵尸进程的流程，在这里 init 进程发现所有子进程都退出了，就会让自己也退出）。\u003c/li\u003e\n\u003cli\u003e（3）具备清理僵尸进程的能力。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e如何拿到正确的CPU开销\u003c/h2\u003e\n\u003cp\u003e在容器中运行 top 命令，虽然可以看到容器中每个进程的 CPU 使用率，但是 top 中\u0026quot;%Cpu(s)\u0026quot;那一行中显示的数值，并不是这个容器的 CPU 整体使用率，而是容器宿主机的 CPU 使用率。\u003c/p\u003e\n\u003cp\u003e对于每个进程，top 都会从 proc 文件系统中每个进程对应的 stat 文件\u003ccode\u003e/proc/[pid]/stat\u003c/code\u003e中读取 2 个数值utime(进程的用户态部分在 Linux 调度中获得 CPU 的 ticks)和stime(进程的内核态部分在 Linux 调度中获得 CPU 的 ticks)\u003c/p\u003e\n\u003cp\u003e由于 /proc/stat 文件是整个节点全局的状态文件，不属于任何一个 Namespace，因此在容器中无法通过读取 /proc/stat 文件来获取单个容器的 CPU 使用率。\u003c/p\u003e\n\u003cp\u003e所以要得到单个容器的 CPU 使用率，我们可以从 CPU Cgroup 每个控制组里的统计文件 cpuacct.stat 中获取。单个容器 CPU 使用率 =((utime_2 – utime_1) + (stime_2 – stime_1)) * 100.0 / (HZ * et * 1 )。\u003c/p\u003e\n\u003ch1\u003e容器内存\u003c/h1\u003e\n\u003ch2\u003enamespace\u003c/h2\u003e\n\u003ch2\u003ecgroup\u003c/h2\u003e\n\u003ch3\u003ememory\u003c/h3\u003e\n\u003cp\u003eMemory Cgroup 的虚拟文件系统的挂载点一般在\u0026quot;/sys/fs/cgroup/memory\u0026quot;这个目录下,可以在 Memory Cgroup 的挂载点目录下，创建一个子目录作为控制组。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ememory.limit_in_bytes: 一个控制组里所有进程可使用内存的最大值\u003c/li\u003e\n\u003cli\u003ememory.oom_control: 当控制组中的进程内存使用达到上限值时，这个参数能够决定会不会触发 OOM Killer(不希望触发 OOM Killer，只要执行 echo 1 \u0026gt; memory.oom_control)\u003c/li\u003e\n\u003cli\u003ememory.usage_in_bytes: 只读的，它里面的数值是当前控制组里所有进程实际使用的内存总和。\u003c/li\u003e\n\u003cli\u003ememory.swappiness: 用于控制是否使用swap分区\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于每个容器创建后，系统都会为它建立一个 Memory Cgroup 的控制组，容器的所有进程都在这个控制组里。\u003c/p\u003e\n\u003cp\u003e容器里所有进程使用的内存量，超过了容器所在 Memory Cgroup 里的内存限制。这时 Linux 系统就会主动杀死容器中的一个进程，往往这会导致整个容器的退出。\u003c/p\u003e\n\u003cp\u003e为什么 memory.usage_in_bytes 与 memory.limit_in_bytes 的值只相差了 90KB，我们在容器中还是可以申请出 50MB 的物理内存？\u003c/p\u003e\n\u003cp\u003eMemory Cgroup 控制组里 RSS 内存和 Page Cache 内存的和，正好是 memory.usage_in_bytes 的值。\u003c/p\u003e\n\u003cp\u003e当控制组里的进程需要申请新的物理内存，而且 memory.usage_in_bytes 里的值超过控制组里的内存上限值 memory.limit_in_bytes，这时我们前面说的 Linux 的内存回收（page frame reclaim）就会被调用起来。\u003c/p\u003e\n\u003cp\u003e那么在这个控制组里的 page cache 的内存会根据新申请的内存大小释放一部分，这样我们还是能成功申请到新的物理内存，整个控制组里总的物理内存开销 memory.usage_in_bytes 还是不会超过上限值 memory.limit_in_bytes。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003edocker run -d --name mem_alloc registry/\u003cspan class=\"hljs-attr\"\u003emem_alloc\u003c/span\u003e:v1\n\nsleep \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n\u003cspan class=\"hljs-variable constant_\"\u003eCONTAINER_ID\u003c/span\u003e=$(sudo docker ps --format \u003cspan class=\"hljs-string\"\u003e\u0026quot;\\{\\{.ID\\}\\}\\t\\{\\{.Names\\}\\}\u0026quot;\u003c/span\u003e | grep -i mem_alloc | awk \u003cspan class=\"hljs-string\"\u003e\u0026#x27;{print $1}\u0026#x27;\u003c/span\u003e)\necho $CONTAINER_ID\n\n\u003cspan class=\"hljs-variable constant_\"\u003eCGROUP_CONTAINER_PATH\u003c/span\u003e=$(find /sys/fs/cgroup/memory/ -name \u003cspan class=\"hljs-string\"\u003e\u0026quot;*$CONTAINER_ID*\u0026quot;\u003c/span\u003e)\necho $CGROUP_CONTAINER_PATH\n\necho \u003cspan class=\"hljs-number\"\u003e536870912\u003c/span\u003e \u0026gt; $CGROUP_CONTAINER_PATH/memory.\u003cspan class=\"hljs-property\"\u003elimit_in_bytes\u003c/span\u003e\ncat $CGROUP_CONTAINER_PATH/memory.\u003cspan class=\"hljs-property\"\u003elimit_in_bytes\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ememory.limit_in_bytes: 设置容器最大的内存使用量\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e如果我们运行docker inspect 命令查看容器退出的原因，就会看到容器处于\u0026quot;exited\u0026quot;状态，并且\u0026quot;OOMKilled\u0026quot;是 true。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e在发生 OOM 的时候,Linux根据进程已经使用的物理内存页面数，每个进程的OOM校准值oom_score_adj(在 /proc 文件系统中，每个进程都有一个 /proc//oom_score_adj 的接口文件。我们可以在这个文件中输入 -1000 到 1000 之间的任意一个数值，调整进程被 OOM Kill 的几率。)来决定杀死哪个进程的，用系统总的可用页面数，去乘以 OOM 校准值 oom_score_adj，再加上进程已经使用的物理页面数，计算出来的值越大，那么这个进程被 OOM Kill 的几率也就越大。\u003c/p\u003e\n\u003ch2\u003eswap分区\u003c/h2\u003e\n\u003cp\u003ememory.swappiness 可以控制这个 Memroy Cgroup 控制组下面匿名内存和 page cache 的回收，取值的范围和工作方式和全局的 swappiness 差不多。这里有一个优先顺序，在 Memory Cgorup 的控制组里，如果你设置了 memory.swappiness 参数，它就会覆盖全局的 swappiness，让全局的 swappiness 在这个控制组里不起作用。\u003c/p\u003e\n\u003cp\u003e不过有一点和全局的swappiness不同的是：当 memory.swappiness = 0 的时候，对匿名页的回收是始终禁止的，也就是始终都不会使用 Swap 空间。\u003c/p\u003e\n\u003cp\u003e这时 Linux 系统不会再去比较 free 内存和 zone 里的 high water mark 的值，再决定一个 Memory Cgroup 中的匿名内存要不要回收了。\u003c/p\u003e\n\u003ch1\u003e容器存储\u003c/h1\u003e\n\u003ch2\u003enamespace\u003c/h2\u003e\n\u003ch3\u003emount\u003c/h3\u003e\n\u003cp\u003e容器中的根文件系统，其实就是我们做的镜像。\u003c/p\u003e\n\u003cp\u003eMount Namespace 保证了每个容器都有自己独立的文件目录结构。\u003c/p\u003e\n\u003cp\u003e可以通过 /proc/mounts 这个路径，找到容器 OverlayFS 对应的 lowerdir 和 upperdir。\u003c/p\u003e\n\u003ch2\u003ecgroups\u003c/h2\u003e\n\u003ch3\u003equota\u003c/h3\u003e\n\u003cp\u003e从宿主机的角度看，upperdir 就是一个目录，如果容器不断往容器文件系统中写入数据，实际上就是往宿主机的磁盘上写数据，这些数据也就存在于宿主机的磁盘目录中。\u003c/p\u003e\n\u003cp\u003e当然对于容器来说，如果有大量的写操作是不建议写入容器文件系统的，一般是需要给容器挂载一个 volume，用来满足大量的文件读写。\u003c/p\u003e\n\u003cp\u003e但是不能避免的是，用户在容器中运行的程序有错误，或者进行了错误的配置。\u003c/p\u003e\n\u003cp\u003e比如说，我们把 log 写在了容器文件系统上，并且没有做 log rotation，那么时间一久，就会导致宿主机上的磁盘被写满。这样影响的就不止是容器本身了，而是整个宿主机了\u003c/p\u003e\n\u003cp\u003e因为 upperdir 在宿主机上也是一个普通的目录, 可以通过限制 upperdir 目录容量的方式，来限制一个容器 OverlayFS 根目录的写入数据量,对于 Linux 上最常用的两个文件系统 XFS 和 ext4，它们有一个特性 Quota,这个特性可以为 Linux 系统里的一个用户（user），一个用户组（group）或者一个项目（project）来限制它们使用文件系统的额度（quota），也就是限制它们可以写入文件系统的文件总量。\u003c/p\u003e\n\u003cp\u003e要使用 XFS Quota 特性，必须在文件系统挂载的时候加上对应的 Quota 选项，比如我们目前需要配置 Project Quota，那么这个挂载参数就是\u0026quot;pquota\u0026quot;。对于根目录来说，这个参数必须作为一个内核启动的参数\u0026quot;rootflags=pquota\u0026quot;，这样设置就可以保证根目录在启动挂载的时候，带上 XFS Quota 的特性并且支持 Project 模式。我们可以从 /proc/mounts 信息里，看看根目录是不是带\u0026quot;prjquota\u0026quot;字段。如果里面有这个字段，就可以确保文件系统已经带上了支持 project 模式的 XFS quota 特性。\u003c/p\u003e\n\u003cp\u003e通过 xfs_quota 这条命令,给一个指定的目录打上一个 Project ID, 未这个id做Quota限制\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003exfs_quota -x -c \u0026#39;limit -p bhard=10m 101\u0026#39; /\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在用 docker run 启动容器的时候，加上一个参数 --storage-opt size= ，就能限制住容器 OverlayFS 文件系统可写入的最大数据量了。\u003c/p\u003e\n\u003ch3\u003eblkio\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDirect I/O可以通过blkio Cgroup来限制磁盘I/O，但是Buffered I/O不能被限制。这个问题只有在cgroupv2版本中才解决了\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e不过容器文件系统并不适合频繁地读写。对于频繁读写的数据，容器需要把他们到放到\u0026quot;volume\u0026quot;中。这里的volume可以是一个本地的磁盘，也可以是一个网络磁盘。如果多个容器同时读写是会产生影响的，我们可以限制某一个的读写速率\u003c/p\u003e\n\u003cp\u003eCgroup v1中有blkio子系统, 可以来限制磁盘的I/O。不过blkio子系统对于磁盘I/O的限制，并不像CPU，Memory那么直接\u003c/p\u003e\n\u003cp\u003e在Cgroups v1里，blkio Cgroup的虚拟文件系统挂载点一般在\u0026quot;/sys/fs/cgroup/blkio/\u0026quot;。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eblkio.throttle.read_iops_device\u003c/li\u003e\n\u003cli\u003eblkio.throttle.read_bps_device\u003c/li\u003e\n\u003cli\u003eblkio.throttle.write_iops_device\u003c/li\u003e\n\u003cli\u003eblkio.throttle.write_bps_device\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# \u0026quot;252:16\u0026quot;是要写入的设备例如/dev/vdb的主次设备号，你可以通过 ls -l /dev/vdb 看到这两个值\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# 设置写入吞吐量不超过10MB/s\u003c/span\u003e\necho \u003cspan class=\"hljs-string\"\u003e\u0026quot;252:16 10485760\u0026quot;\u003c/span\u003e \u0026gt; $CGROUP_CONTAINER_PATH/blkio.throttle.write_bps_device\n\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003eio\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003ecgroupv2 版本中的系统，用于解决无法对buffered io限速的问题\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Create a new control group\u003c/span\u003e\nmkdir -p /sys/fs/cgroup/unified/iotest\n\n\u003cspan class=\"hljs-comment\"\u003e# enable the io and memory controller subsystem\u003c/span\u003e\necho \u003cspan class=\"hljs-string\"\u003e\u0026quot;+io +memory\u0026quot;\u003c/span\u003e \u0026gt; /sys/fs/cgroup/unified/cgroup.subtree_control\n\n\u003cspan class=\"hljs-comment\"\u003e# Add current bash pid in iotest control group.\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# Then all child processes of the bash will be in iotest group too,\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# including the fio\u003c/span\u003e\n\necho $$ \u0026gt;/sys/fs/cgroup/unified/iotest/cgroup.procs\n\n\u003cspan class=\"hljs-comment\"\u003e# 256:16 are device major and minor ids, /mnt is on the device.\u003c/span\u003e\necho \u003cspan class=\"hljs-string\"\u003e\u0026quot;252:16 wbps=10485760\u0026quot;\u003c/span\u003e \u0026gt; /sys/fs/cgroup/unified/iotest/io.\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e\ncd /mnt\n\u003cspan class=\"hljs-comment\"\u003e#Run the fio in non direct I/O mode\u003c/span\u003e\nfio -iodepth=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e  -name=./fio.test\n\u003c/code\u003e\u003c/pre\u003e\u003ch2\u003e容器镜像的文件系统\u003c/h2\u003e\n\u003cp\u003e在容器里，运行 df 命令，你可以看到在容器中根目录 (/) 的文件系统类型是\u0026quot;overlay\u0026quot;，它不是我们在普通 Linux 节点上看到的 Ext4 或者 XFS 之类常见的文件系统。\u003c/p\u003e\n\u003cp\u003e如果没有特别的容器文件系统，只是普通的 Ext4 或者 XFS 文件系统，那么每次启动一个容器，就需要把一个镜像文件下载并且存储在宿主机上。overlay文件系统就是用来解决这种冗余的\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在 Linux 内核 3.18 版本中，OverlayFS 代码正式合入 Linux 内核的主分支。在这之后，OverlayFS 也就逐渐成为各个主流 Linux 发行版本里缺省使用的容器文件系统了。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eoverlay在分类中属于UnionFS，UnionFS 这类文件系统实现的主要功能是把多个目录（处于不同的分区）一起挂载（mount）在一个目录下。\u003c/p\u003e\n\u003cimg src=\"/images/blogs/unionfs示意图.jpeg\" /\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u0026quot;work/\u0026quot;，一个存放临时文件的目录，OverlayFS 中如果有文件修改，就会在中间过程中临时存放文件到这里。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# !/bin/bash\u003c/span\u003e\n\numount ./merged\nrm upper lower merged work -r\n\nmkdir upper lower merged work\necho \u003cspan class=\"hljs-string\"\u003e\u0026quot;I\u0026#x27;m from lower!\u0026quot;\u003c/span\u003e \u0026gt; lower/in_lower.txt\necho \u003cspan class=\"hljs-string\"\u003e\u0026quot;I\u0026#x27;m from upper!\u0026quot;\u003c/span\u003e \u0026gt; upper/in_upper.txt\n\u003cspan class=\"hljs-comment\"\u003e# `in_both` is in both directories\u003c/span\u003e\necho \u003cspan class=\"hljs-string\"\u003e\u0026quot;I\u0026#x27;m from lower!\u0026quot;\u003c/span\u003e \u0026gt; lower/in_both.txt\necho \u003cspan class=\"hljs-string\"\u003e\u0026quot;I\u0026#x27;m from upper!\u0026quot;\u003c/span\u003e \u0026gt; upper/in_both.txt\n\nsudo mount -t overlay overlay \\\n -o lowerdir=./lower,upperdir=./upper,workdir=./work \\\n ./merged\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eOverlayFS 也是把多个目录合并挂载，被挂载的目录分为两大类：lowerdir 和 upperdir。\u003c/p\u003e\n\u003cp\u003elowerdir 允许有多个目录，在被挂载后，这些目录里的文件都是不会被修改或者删除的，也就是只读的；upperdir 只有一个，不过这个目录是可读写的，挂载点目录中的所有文件修改都会在 upperdir 中反映出来。\u003c/p\u003e\n\u003cp\u003e在merged文件夹中进行操作的效果:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e新建文件，这个文件会出现在 upper/ 目录中。\u003c/li\u003e\n\u003cli\u003e删除文件，如果我们删除\u0026quot;in_upper.txt\u0026quot;，那么这个文件会在 upper/ 目录中消失。如果删除\u0026quot;in_lower.txt\u0026quot;, 在 lower/ 目录里的\u0026quot;in_lower.txt\u0026quot;文件不会有变化，只是在 upper/ 目录中增加了一个特殊文件来告诉 OverlayFS，\u0026quot;in_lower.txt\u0026#39;这个文件不能出现在 merged/ 里了，这就表示它已经被删除了。\u003c/li\u003e\n\u003cli\u003e修改文件, 类似如果修改\u0026quot;in_lower.txt\u0026quot;，那么就会在 upper/ 目录中新建一个\u0026quot;in_lower.txt\u0026quot;文件，包含更新的内容，而在 lower/ 中的原来的实际文件\u0026quot;in_lower.txt\u0026quot;不会改变。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e磁盘读写\u003c/h2\u003e\n\u003cp\u003eBlkio Cgroup: 在容器中对磁盘I/O的限速\u003c/p\u003e\n\u003cp\u003eLinux内存和文件系统管理对容器IO的影响: Page Frame Reclaim, Dirty Page\u003c/p\u003e\n\u003ch1\u003e容器网络\u003c/h1\u003e\n\u003ch2\u003enamespace\u003c/h2\u003e\n\u003ch3\u003enetwork\u003c/h3\u003e\n\u003cp\u003e隔离的资源包括\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e第一种，网络设备，lo，eth0等\u003c/li\u003e\n\u003cli\u003e第二种是 IPv4 和 IPv6 协议栈。IP 层以及上面的 TCP 和 UDP 协议栈也是每个 Namespace 独立工作的。所以 IP、TCP、UDP 的很多协议，它们的相关参数也是每个 Namespace 独立的，这些参数大多数都在 /proc/sys/net/ 目录下面，同时也包括了 TCP 和 UDP 的 port 资源。\u003c/li\u003e\n\u003cli\u003e第三种，IP 路由表\u003c/li\u003e\n\u003cli\u003e第四种是iptables 规则了\u003c/li\u003e\n\u003cli\u003e第五种是网络的状态信息，这些信息你可以从 /proc/net 和 /sys/class/net 里得到\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003etcp_keepalive 的三个参数都是重新初始化的，而 tcp_congestion_control 的值是从 Host Namespace 里复制过来的。\u003c/p\u003e\n\u003cp\u003e在启动普通容器，尝试一下在容器里去修改\u0026quot;/proc/sys/net/\u0026quot;下的参数，会报错\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e只读模式下，容器中\u0026quot;/proc/sys/\u0026quot;是只读 mount 的，那么在容器里是不能修改\u0026quot;/proc/sys/net/\u0026quot;下面的任何参数了。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e想修改 Network Namespace 里的网络参数，要选择容器刚刚启动，而容器中的应用程序还没启动之前进行。runC 也在对 /proc/sys 目录做 read-only mount 之前，预留出了修改接口，就是用来修改容器里 \u0026quot;/proc/sys\u0026quot;下参数的，同样也是 sysctl 的参数。而 Docker 的–sysctl或者 Kubernetes 里的allowed-unsafe-sysctls特性也都利用了 runC 的 sysctl 参数修改接口，允许容器在启动时修改容器 Namespace 里的参数。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# docker run -d --name net_para --sysctl net.ipv4.tcp_keepalive_time=600 centos:8.1.1911 sleep 3600\u003c/span\u003e\n7efed88a44d64400ff5a6d38fdcc73f2a74a7bdc3dbc7161060f2f7d0be170d1\n\u003cspan class=\"hljs-comment\"\u003e# docker exec net_para cat /proc/sys/net/ipv4/tcp_keepalive_time\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e600\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003cimg src=\"/images/blogs/networknamespace工具包.jpeg\" /\u003e\n\n\u003ch2\u003ecgroup\u003c/h2\u003e\n\u003ch2\u003e容器网络接口\u003c/h2\u003e\n\u003cimg src=\"/images/blogs/network网络模式.jpeg\" /\u003e\n\n\u003cp\u003e第一步，就是要让数据包从容器的 Network Namespace 发送到 Host Network Namespace 上。\u003c/p\u003e\n\u003cp\u003e一般来说就只有两类设备接口：一类是veth，另外一类是 macvlan/ipvlan。\u003c/p\u003e\n\u003cp\u003e用 Docker 启动的容器缺省的网络接口用的也是这个 veth\u003c/p\u003e\n\u003cimg src=\"/images/blogs/veth.jpeg\" /\u003e\n\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# 示例 创建一个无网络的容器，手动创建一个veth网卡\u003c/span\u003e\ndocker run -d --name \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e-test --network none centos:\u003cspan class=\"hljs-number\"\u003e8.1\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.1911\u003c/span\u003e sleep \u003cspan class=\"hljs-number\"\u003e36000\u003c/span\u003e\n\npid=$(ps -ef | grep \u003cspan class=\"hljs-string\"\u003e\u0026quot;sleep 36000\u0026quot;\u003c/span\u003e | grep -v grep | awk \u003cspan class=\"hljs-string\"\u003e\u0026#x27;{print $2}\u0026#x27;\u003c/span\u003e)\necho $pid\n\n\u003cspan class=\"hljs-comment\"\u003e# 在\u0026quot;/var/run/netns/\u0026quot;的目录下建立一个符号链接，指向这个容器的 Network Namespace。完成这步操作之后，在后面的\u0026quot;ip netns\u0026quot;操作里，就可以用 pid 的值作为这个容器的 Network Namesapce 的标识了。\u003c/span\u003e\nln -s /proc/$pid/ns/net /var/run/netns/$pid\n\n\u003cspan class=\"hljs-comment\"\u003e# 建立一对 veth 的虚拟设备接口\u003c/span\u003e\nip link add name veth_host \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e veth peer name veth_container\n\u003cspan class=\"hljs-comment\"\u003e# 把 veth_container 这个接口放入到容器的 Network Namespace 中\u003c/span\u003e\nip link \u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e veth_container netns $pid\n\n\u003cspan class=\"hljs-comment\"\u003e# In the container, setup veth_container\u003c/span\u003e\nip netns \u003cspan class=\"hljs-built_in\"\u003eexec\u003c/span\u003e $pid ip link \u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e veth_container name eth0\nip netns \u003cspan class=\"hljs-built_in\"\u003eexec\u003c/span\u003e $pid ip addr add \u003cspan class=\"hljs-number\"\u003e172.17\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e dev eth0\nip netns \u003cspan class=\"hljs-built_in\"\u003eexec\u003c/span\u003e $pid ip link \u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e eth0 up\nip netns \u003cspan class=\"hljs-built_in\"\u003eexec\u003c/span\u003e $pid ip route add default via \u003cspan class=\"hljs-number\"\u003e172.17\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.0\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# In the host, set veth_host up\u003c/span\u003e\nip link \u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e veth_host up\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eipvlan网络配置方式\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e对于延时敏感的应用程序，我们可以考虑使用 ipvlan/macvlan 网络接口的容器。不过，由于 ipvlan/macvlan 网络接口直接挂载在物理网络接口上，对于需要使用 iptables 规则的容器，比如 Kubernetes 里使用 service 的容器，就不能工作了。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e无论是 macvlan 还是 ipvlan，它们都是在一个物理的网络接口上再配置几个虚拟的网络接口。在这些虚拟的网络接口上，都可以配置独立的 IP，并且这些 IP 可以属于不同的 Namespace。对于 macvlan，每个虚拟网络接口都有自己独立的 mac 地址；而 ipvlan 的虚拟网络接口是和物理网络接口共享同一个 mac 地址。而且它们都有自己的 L2/L3 的配置方式\u003c/p\u003e\n\u003cimg src=\"/images/blogs/ipvlan.jpeg\" /\u003e\n\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003edocker run --init --name lat-test-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e --network none -d registry/latency-test:v1 sleep \u003cspan class=\"hljs-number\"\u003e36000\u003c/span\u003e\n\npid1=$(docker inspect lat-test-\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e | grep -i Pid | head -n \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e | awk \u003cspan class=\"hljs-string\"\u003e\u0026#x27;{print $2}\u0026#x27;\u003c/span\u003e | awk -F \u003cspan class=\"hljs-string\"\u003e\u0026quot;,\u0026quot;\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\u0026#x27;{print $1}\u0026#x27;\u003c/span\u003e)\necho $pid1\nln -s /proc/$pid1/ns/net /var/run/netns/$pid1\n \nip link add link eth0 ipvt1 \u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e ipvlan mode l2\nip link \u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e dev ipvt1 netns $pid1\n\nip netns \u003cspan class=\"hljs-built_in\"\u003eexec\u003c/span\u003e $pid1 ip link \u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e ipvt1 name eth0\nip netns \u003cspan class=\"hljs-built_in\"\u003eexec\u003c/span\u003e $pid1 ip addr add \u003cspan class=\"hljs-number\"\u003e172.17\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e dev eth0\nip netns \u003cspan class=\"hljs-built_in\"\u003eexec\u003c/span\u003e $pid1 ip link \u003cspan class=\"hljs-built_in\"\u003eset\u003c/span\u003e eth0 up\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e第二步，数据包发到了 Host Network Namespace 之后，还要解决数据包怎么从宿主机上的 eth0 发送出去的问题。\u003c/p\u003e\n\u003cp\u003e这一步呢，就是一个普通 Linux 节点上数据包转发的问题了。用 nat 来做个转发，或者建立 Overlay 网络发送，也可以通过配置 proxy arp 加路由的方法来实现。\u003c/p\u003e\n\u003cp\u003eDocker 缺省使用的是 bridge + nat 的转发方式\u003c/p\u003e\n\u003cp\u003eDocker 程序在节点上安装完之后，就会自动建立了一个 docker0 的 bridge interface。所以我们只需要把第一步中建立的 veth_host 这个设备，接入到 docker0 这个 bridge 上。\u003c/p\u003e\n\u003cimg src=\"/images/blogs/网络第二步.png\" /\u003e\n\n\u003cp\u003e容器和 docker0 组成了一个子网，docker0 上的 IP 就是这个子网的网关 IP。如果我们要让子网通过宿主机上 eth0 去访问外网的话，那么加上 iptables 的规则就可以了\u003ccode\u003eiptables -P FORWARD ACCEPT\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e两个网络设备接口之间的数据包转发，需要ip_forward参数，所以还需要把这个改为1\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003eecho \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e \u0026gt; \u003cspan class=\"hljs-regexp\"\u003e/proc/\u003c/span\u003esys/net/ipv4/ip_forward\n\u003c/code\u003e\u003c/pre\u003e\u003ch2\u003e容器网络的问题\u003c/h2\u003e\n\u003cp\u003e延时增加, 因为多了虚拟网卡的转发以及各种路由\u003c/p\u003e\n\u003cp\u003e乱序包的增加\u003c/p\u003e\n\u003cp\u003e在云平台的这种网络环境里，网络包乱序 +SACK 之后，产生的数据包重传的量要远远高于网络丢包引起的重传。\u003c/p\u003e\n\u003cp\u003e把 RPS 的这个特性配置到 veth 网络接口上，来减少数据包乱序的几率。不过RPS 的配置还是会带来额外的系统开销，在某些网络环境中会引起 softirq CPU 使用率的增大\u003c/p\u003e\n\u003cp\u003e尽管容器中 root 用户的 Linux capabilities 已经减少了很多，但是在没有 User Namespace 的情况下，容器中 root 用户和宿主机上的 root 用户的 uid 是完全相同的，一旦有软件的漏洞，容器中的 root 用户就可以操控整个宿主机。\u003c/p\u003e\n\u003cp\u003eUser Namespace，它带来的好处有两个。一个是把容器中 root 用户（uid 0）映射成宿主机上的普通用户，另外一个好处是在云平台里对于容器 uid 的分配要容易些。\u003c/p\u003e\n\u003cp\u003e为了减少安全风险，业界都是建议在容器中以非 root 用户来运行进程。不过在没有 User Namespace 的情况下，在容器中使用非 root 用户，对于容器云平台来说，对 uid 的管理会比较麻烦。\u003c/p\u003e\n\u003cp\u003e除了在容器中以非 root 用户来运行进程外，Docker 和 podman 都支持了 rootless container，也就是说它们都可以以非 root 用户来启动和管理容器，这样就进一步降低了容器的安全风险。\u003c/p\u003e\n\u003ch1\u003e容器安全\u003c/h1\u003e\n\u003cp\u003e对于容器的 root 用户，缺省只赋予了 15 个 capabilities。如果我们发现容器中进程的权限不够，就需要分析它需要的最小 capabilities 集合，而不是直接赋予容器\u0026quot;privileged\u0026quot;。\u003c/p\u003e\n\u003cp\u003e因为\u0026quot;privileged\u0026quot;包含了所有的 Linux capabilities, 这样\u0026quot;privileged\u0026quot;就可以轻易获取宿主机上的所有资源，这会对宿主机的安全产生威胁。容器平台上是基本不允许把容器直接设置为\u0026quot;privileged\u0026quot;的，我们需要根据容器中进程需要的最少特权来赋予 capabilities。\u003c/p\u003e\n\u003cp\u003e假设容器里需要使用 iptables。因为使用 iptables 命令，只需要设置 CAP_NET_ADMIN 这个 capability 就行。那么我们只要在运行 Docker 的时候，给这个容器再多加一个 NET_ADMIN 参数就可以了。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-comment\"\u003e# docker run --name iptables --cap-add NET_ADMIN -it registry/iptables:v1 bash\u003c/span\u003e\n[root@cfedf124dcf1 /]\u003cspan class=\"hljs-comment\"\u003e# iptables -L\u003c/span\u003e\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\n\u003c/code\u003e\u003c/pre\u003e\u003ch2\u003enot a root\u003c/h2\u003e\n\u003cp\u003e如果不想让容器以 root 用户运行，最直接的办法就是给容器指定一个普通用户 uid\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-string\"\u003edocker\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003erun\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-ti\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e--name\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eroot_example\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-u\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e6667\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e:6667\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-v\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e/etc:/mnt\u003c/span\u003e  \u003cspan class=\"hljs-string\"\u003ecentos\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ebash\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e还有另外一个办法，就是我们在创建容器镜像的时候，用 Dockerfile 为容器镜像里建立一个用户。这样操作以后，容器里缺省的进程都会以这个用户启动。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-dockerfile\"\u003e\u003cspan class=\"hljs-string\"\u003eFROM\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ecentos\u003c/span\u003e\n\n\u003cspan class=\"hljs-string\"\u003eRUN\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eadduser\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e-u\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e6667 \u003c/span\u003e\u003cspan class=\"hljs-string\"\u003enonroot\u003c/span\u003e\n\u003cspan class=\"hljs-string\"\u003eUSER\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003enonroot\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e但是这种方法会导致当多个客户的容器在同一个节点上运行的时候，其实就都使用了宿主机上 uid 6667。然而Linux 系统上，每个用户下的资源是有限制的，比如打开文件数目（open files）、最大进程数目（max user processes）等等。一旦有很多个容器共享一个 uid，这些容器就很可能很快消耗掉这个 uid 下的资源，这样很容易导致这些容器都不能再正常工作。\u003c/p\u003e\n\u003ch2\u003eUsernamespace\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003ek8s还未引入\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eUser Namespace 隔离了一台 Linux 节点上的 User ID（uid）和 Group ID（gid），它给 Namespace 中的 uid/gid 的值与宿主机上的 uid/gid 值建立了一个映射关系。经过 User Namespace 的隔离，我们在 Namespace 中看到的进程的 uid/gid，就和宿主机 Namespace 中看到的 uid 和 gid 不一样了。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e跟 Docker 相比，podman 不再有守护进程 dockerd，而是直接通过 fork/execve 的方式来启动一个新的容器。这种方式启动容器更加简单，也更容易维护。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003ccode\u003epodman run -ti  -v /etc:/mnt --uidmap 0:2000:1000 centos bash\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e0:2000:1000\u003c/code\u003e意思是第一个 0 是指在新的 Namespace 里 uid 从 0 开始，中间的那个 2000 指的是 Host Namespace 里被映射的 uid 从 2000 开始，最后一个 1000 是指总共需要连续映射 1000 个 uid。\u003c/p\u003e\n\u003cp\u003e容器里的uid 0 是被映射到宿主机上的 uid 2000\u003c/p\u003e\n\u003ch2\u003erootless container\u003c/h2\u003e\n\u003cp\u003erootless container 中的\u0026quot;rootless\u0026quot;不仅仅指容器中以非 root 用户来运行进程，还指以非 root 用户来创建容器，管理容器。也就是说，启动容器的时候，Docker 或者 podman 是以非 root 用户来执行的。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e$ \u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e\nuid=\u003cspan class=\"hljs-number\"\u003e1001\u003c/span\u003e(redhat) gid=\u003cspan class=\"hljs-number\"\u003e1001\u003c/span\u003e(redhat) groups=\u003cspan class=\"hljs-number\"\u003e1001\u003c/span\u003e(redhat)\n$ podman run -it  ubi7/ubi bash   \u003cspan class=\"hljs-comment\"\u003e### 在宿主机上以redhat用户启动容器\u003c/span\u003e\n[root@206f6d5cb033 /]\u003cspan class=\"hljs-comment\"\u003e# id     ### 容器中的用户是root\u003c/span\u003e\nuid=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e(root) gid=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e(root) groups=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e(root)\n[root@206f6d5cb033 /]\u003cspan class=\"hljs-comment\"\u003e# sleep 3600   ### 在容器中启动一个sleep 进程\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# ps -ef |grep sleep   ###在宿主机上查看容器sleep进程对应的用户\u003c/span\u003e\nredhat   \u003cspan class=\"hljs-number\"\u003e29433\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e29410\u003c/span\u003e  \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e 05:\u003cspan class=\"hljs-number\"\u003e14\u003c/span\u003e pts/\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e    \u003cspan class=\"hljs-number\"\u003e00\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e00\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e00\u003c/span\u003e sleep \u003cspan class=\"hljs-number\"\u003e3600\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e","frontMatter":{"readingTime":{"text":"46 min read","minutes":45.75,"time":2745000,"words":9150},"slug":"容器核心点概览","fileName":"容器核心点概览.md","title":"容器核心点","date":"2022-08-14T00:00:00.000Z","tags":["容器"],"draft":false,"summary":"容器使用与主机的区别"}},"prev":{"title":"grpc手册","date":"2022-08-13T00:00:00.000Z","tags":["中间件"],"draft":false,"summary":"grpc介绍以及使用","slug":"grpc手册"},"next":{"title":"ssh环境配置","date":"2022-10-08T00:00:00.000Z","tags":["实践"],"draft":false,"summary":"ssh环境配置","slug":"ssh环境配置"}},"__N_SSG":true},"page":"/blog/[...slug]","query":{"slug":["容器核心点概览"]},"buildId":"PMB5qMppIrri5HN2Gb314","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>