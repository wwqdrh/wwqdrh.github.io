{"pageProps":{"post":{"mdxSource":"<img src=\"/images/blogs/container.png\" />\n\n<h1>底层原理</h1>\n<p>基于Linux内核的Cgroup、Namespace、UnionFS等技术，对进程进行封装隔离(本质上还是一个进程)，用于操作系统层面的虚拟化技术</p>\n<h2>Namespace</h2>\n<blockquote>\n<p>参考资料\n<a href=\"https://en.wikipedia.org/wiki/Linux_namespaces\">https://en.wikipedia.org/wiki/Linux_namespaces</a>\n<a href=\"https://man7.org/linux/man-pages/man7/namespaces.7.html\">https://man7.org/linux/man-pages/man7/namespaces.7.html</a></p>\n</blockquote>\n<p>对namespace的操作方法</p>\n<p><code>clone</code></p>\n<p>在创建新进程的系统调用时，通过CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET等创建新的命名空间</p>\n<p><code>setns</code></p>\n<p>让一个调用进程加入某个已经存在的Namespace中</p>\n<p><code>unshare</code></p>\n<p>将调用进程移动到新的Namespace下, 各个namespace下的资源配置则是通过各种容器管理工具(docker、kubernetes)，例如通过<code>unshare -fn [进程命令]</code>,创建的network namespace就是只有loopback</p>\n<p>用用命令</p>\n<ul>\n<li>查看当前系统所在命名空间: <code>lsns -t [类型]</code></li>\n<li>查看某进程的namespace: <code>ls -la /proc/&lt;pid&gt;/ns/</code></li>\n<li>进入某namespace运行命令: <code>nsenter -t &lt;pid&gt; -n ip addr</code></li>\n</ul>\n<blockquote>\n<p>提供共不同类型资源的隔离</p>\n</blockquote>\n<p>内核级别的系统资源隔离方式</p>\n<p>主机有其默认的Namespace，行为与用户的Namespace 完全一致，不同进程可以通过不同的Namespace 进行隔离。</p>\n<p>当系统启动一个容器时，会为该容器创建相应的不同类型的Namespace，为运行在容器内的进程提供相应的资源隔离。</p>\n<p>每个Namespace都附带一个编号，是该Namespace在系统中的唯一标识。</p>\n<p>对Namespace操作相关的系统调用</p>\n<ul>\n<li>clone：创建新进程的系统调用时，通过flags参数指定需要新建的Namespace类型</li>\n<li>setns：让调用进程加入某个已经存在的Namespace</li>\n<li>unshare：将调用进程移动到新的Namespace下</li>\n</ul>\n<h3>Mount (mnt)</h3>\n<p>挂载点</p>\n<ul>\n<li><code>/proc/[pid]/ns/mnt</code></li>\n</ul>\n<blockquote>\n<p>kernel 2.4.19</p>\n</blockquote>\n<p>挂载点</p>\n<p>在主机上<code>/proc/[pid]/mounts</code>，<code>/proc/[pid]/mountinfo</code>(可以看到该pid所属的mount Namespace下的挂载点的传播类型及所属的对等组)，<code>/proc/[pid]/mountstats</code>等文件查看挂载点。</p>\n<p>在容器内，通过mount或lsmnt命令查看Mount Namespace的有效挂载点。</p>\n<p>挂载点传播类型</p>\n<ul>\n<li>MS_SHARED</li>\n<li>MS_PRIVATE</li>\n<li>MS_SLAVE</li>\n<li>MS_UNBINDABLE</li>\n</ul>\n<h3>Process ID (pid)</h3>\n<p>进程id</p>\n<ul>\n<li><code>/proc/[pid]/ns/pid</code></li>\n<li><code>/proc/[pid]/ns/pid_for_children</code></li>\n</ul>\n<blockquote>\n<p>kernel 2.6.14</p>\n</blockquote>\n<p>进程，在主机的<code>/proc/[pid]/ns</code>目录中可以查看系统进程归属的Namespace。</p>\n<p>容器启动后，Entrypoint 进程会作为PID 为1 的进程存在，因此是该PID Namespace 的init 进程。</p>\n<p>它是当前Namespace所有进程的父进程，如果该进程退出，内核会对该PID Namespace的所有进程发送SIGKILL 信号，以便同时结束它们。</p>\n<p>init 进程默认屏蔽系统信号，即除非该进程对系统信号做特殊处理，否则发往该进程的系统信号默认都会被忽略。不过SIGKILL 和SIGSTOP 信号比较特殊，init 进程无法捕获这两个信号。</p>\n<p>Kubernetes 默认对同一Pod 的不同容器构建独立的PIDNamespace，以便将不同容器的进程彼此隔离，同时允许通过ShareProcessNamespace 属性设置不同容器的进程共享PID Namespace。</p>\n<p>Kubernetes 支持多重容器进程的重启策略，默认行为是用户进程退出后立即重启。Kubernetes 用户只需中止其容器中的Entrypoint 进程，即可实现容器重启。</p>\n<h3>Network (net)</h3>\n<ul>\n<li><code>/proc/[pid]/ns/net</code></li>\n</ul>\n<blockquote>\n<p>kernel 2.6.29</p>\n</blockquote>\n<p>一个物理的网络设备通常会放到系统初始的Network Namespace中。</p>\n<ul>\n<li>网络设备</li>\n<li>网络协议栈</li>\n<li>网络端口</li>\n<li>IP路由表</li>\n<li>防火墙规则</li>\n<li><code>/proc/net</code></li>\n<li><code>/sys/class/net</code></li>\n<li><code>/proc/sys/net</code></li>\n</ul>\n<p>不同网络的Namespace 由网络虚拟设备（Virtual EthernetDevice，即VETH）连通，再基于网桥或者路由实现与物理网络设备的连通。当网络Namespace 被释放后，对应的VETH Pair 设备也会被自动释放。</p>\n<p>在Kubernetes 中，同一Pod 的不同容器共享同一网络的Namespace，没有例外。这使得Kubernetes 能将网络挂载在更轻量、更稳定的sandbox 容器上，而用户定义的容器只需复用已配置好的网络即可。另外，同一Pod 的不同容器中运行的进程可以基于localhost彼此通信</p>\n<h3>Interprocess Communication (ipc)</h3>\n<ul>\n<li><code>/proc/[pid]/ns/ipc</code></li>\n</ul>\n<blockquote>\n<p>kernel 2.6.19</p>\n</blockquote>\n<p>System V IPC和POSIX消息队列（都是进程间通信相关的资源）</p>\n<p>System V IPC 对象包含信号量、共享内存和消息队列，用于进程间的通信。System V IPC对象具有全局唯一的标识，对在该IPC Namespace 内的进程可见，而对其外的进程不可见。当IPC Namespace 被销毁后，所有的IPC 对象也会被自动销毁。</p>\n<p>Kubernetes 允许用户在Pod 中使用hostIPC 进行定义，通过该属性使授权用户容器共享主机IPC Namespace，达到进程间通信的目的。</p>\n<h3>UTS</h3>\n<ul>\n<li><code>/proc/[pid]/ns/uts</code></li>\n</ul>\n<blockquote>\n<p>kernel 2.6.19</p>\n</blockquote>\n<p>主机名和域名，UTS Namespace中的一个进程可以看做一个在网络上独立存在的节点（除IP外，还能通过主机名进行访问）</p>\n<h3>User ID (user)</h3>\n<p>用户用户组</p>\n<ul>\n<li><code>/proc/[pid]/ns/user</code></li>\n</ul>\n<blockquote>\n<p>kernel 3.8</p>\n</blockquote>\n<blockquote>\n<p>权限隔离和用户身份标识隔离，在容器类创建切换用户，以及为文件目录设置不同的用户权限，从而实现容器内的权限管理，而无须影响主机配置。</p>\n</blockquote>\n<p>主要隔离安全相关的标识符和属性，用户ID、用户组ID、root 目录、秘钥等。</p>\n<p>一个进程的用户ID 和组ID 在User Namespace 内外可以有所不同。在该User Namespace 外，它是一个非特权的用户ID；而在User Namespace 内，进程可以使用0（root）作为用户ID，且其具有完全的特权权限。</p>\n<h3>Control group (cgroup) Namespace</h3>\n<ul>\n<li><code>/proc/[pid]/ns/cgroup</code></li>\n</ul>\n<h3>Time Namespace</h3>\n<ul>\n<li><code>/proc/[pid]/ns/time</code></li>\n<li><code>/proc/[pid]/ns/time_for_children</code></li>\n</ul>\n<h2>cgroup</h2>\n<p>Linux下用于对一个或一组进程进行资源控制和监控的机制，可以对诸如CPU使用时间、内存、磁盘I/O等进程所需的资源进行限制。</p>\n<p>不同资源的具体管理工作由对应的Cgroup子系统来实现，以层级树(Hierarchy)的方式来组织管理，每个Cgroup都可以包含其他的子Group，因此子Cgroup能使用的资源除了受本Cgroup配置的资源参数限制，还受到父CGroup设置的资源限制，针对不同类型的资源限制，只要讲限制策略在不同的子系统上进行关联即可</p>\n<blockquote>\n<p>kernel 4.6</p>\n</blockquote>\n<p>内核从 4.6 版本开始支持 CGroup Namespace。如果容器启动时没有开启 CGroup Namespace，那么在容器内部查询 CGroup 时，返回整个系统的信息；而开启 CGroup Namespace 后，可以看到当前容器以根形式展示的单独的CGroup 信息。</p>\n<h3>cgroup v1</h3>\n<img src=\"/images/blogs/groupv1.jpeg\" />\n\n\n<p>v1 的CPU Cgroup，memory Cgroup和blkio Cgroup，那么Cgroup v1的一个整体结构，你应该已经很熟悉了。它的每一个子系统都是独立的，资源的限制只能在子系统中发生。</p>\n<h3>cgroup v2</h3>\n<img src=\"/images/blogs/groupv2.jpeg\" />\n\n<p>Cgroup v2相比Cgroup v1做的最大的变动就是一个进程属于一个控制组，而每个控制组里可以定义自己需要的多个子系统。</p>\n<p>比如Cgroup V2中，某个进程pid_y属于控制组group2，而在group2里同时打开了io和memory子系统 （Cgroup V2里的io子系统就等同于Cgroup v1里的blkio子系统）。</p>\n<p>那么，Cgroup对进程pid_y的磁盘 I/O做限制的时候，就可以考虑到进程pid_y写入到Page Cache内存的页面了，这样buffered I/O的磁盘限速就实现了。</p>\n<p>目前即使最新版本的Ubuntu Linux或者Centos Linux，仍然在使用Cgroup v1作为缺省的Cgroup。打开方法就是配置一个kernel参数&quot;cgroup_no_v1=blkio,memory&quot;，这表示把Cgroup v1的blkio和Memory两个子系统给禁止，我们可以把这个参数配置到grub中，然后我们重启Linux机器，这时Cgroup v2的 io还有Memory这两个子系统，它们的功能就打开了。</p>\n<h3>docker中对cgroup的应用</h3>\n<img src=\"/images/blogs/cgroup-docker.png\" />\n\n<h3>CPU</h3>\n<ul>\n<li>cpu.shares</li>\n<li>cpu.cfs_period_us和cpu.cfs_quota_us</li>\n<li>cpu.stat:</li>\n<li>nr_periods</li>\n<li>nr_throttled</li>\n</ul>\n<h3>cpuacct</h3>\n<p>统计CGroup及其子CGroup下进程的CPPU使用情况</p>\n<ul>\n<li>cpuacct.usage</li>\n<li>cpuacct.stat</li>\n</ul>\n<h3>cpuset</h3>\n<p>分配单独的CPU和内存节点，将进程固定在某个CPU或内存节点上，以达到提高性能的目的</p>\n<ul>\n<li>cpuset.cpus</li>\n<li>cpuset.mems</li>\n<li>cpuset.memory_migrate</li>\n<li>cpuset.cpu_exclusive</li>\n<li>cpuset.mem_exclusive</li>\n</ul>\n<h3>memory</h3>\n<p>用于限制CGroup下进程的内存使用量</p>\n<ul>\n<li>memory.stat</li>\n<li>memory.usage_in_bytes</li>\n<li>memory.max_usage_in_bytes</li>\n<li>memory.limit_in_bytes</li>\n<li>memory.failcnt</li>\n<li>memory.force_empty</li>\n<li>memory.oom_control</li>\n</ul>\n<h3>blkio</h3>\n<p>对块设备放的IO控制，按权重分配目前有两种限制方式：一是限制每秒写入的字节数（Bytes Per Second，即BPS），二是限制每秒的读写次数（I/O Per Second，即IOPS）</p>\n<p>依赖于磁盘的CFQ调度，如果磁盘调度使用deadline或者none的算法则无法支持</p>\n<p><strong>权重方式的配置</strong></p>\n<ul>\n<li>blkio.weight</li>\n<li>blkio.weight_device</li>\n</ul>\n<p><strong>IO限流方式的配置</strong></p>\n<p>![[Pasted image 20220524111700.png]]</p>\n<h3>PID</h3>\n<p>PID 子系统用来限制CGroup 能够创建的进程数。</p>\n<p>● pids.max：允许创建的最大进程数量。\n● pids.current：当前的进程数量。</p>\n<h3>其他</h3>\n<p>● devices 子系统，控制进程访问某些设备\n● perf_event 子系统，控制perf 监控CGroup 下的进程。\n● net_cls 子系统，标记CGroups 中进程的网络数据包，通过TC 模块（Traffic Control）对数据包进行控制。\n● net_prio 子系统，针对每个网络设备设置特定的优先级。\n● hugetlb 子系统，对hugepage 的使用进行限制。\n● freezer 子系统，挂起或者恢复CGroups 中的进程。\n● ns 子系统，使不同CGroups 下面的进程使用不同的Namespace。\n● rdma 子系统，对RDMA/IB-spe-cific 资源进行限制</p>\n<h1>容器进程</h1>\n<h2>namespace</h2>\n<h3>pids</h3>\n<img src=\"/images/blogs/namespace-pid.jpeg\" />\n\n<h2>cgroup</h2>\n<h3>pids</h3>\n<p>pids Cgroup 通过 Cgroup 文件系统的方式向用户提供操作接口，一般它的 Cgroup 文件系统挂载点在 /sys/fs/cgroup/pids。</p>\n<p>在一个容器建立之后，创建容器的服务会在 /sys/fs/cgroup/pids 下建立一个子目录，就是一个控制组，控制组里最关键的一个文件就是 pids.max。我们可以向这个文件写入数值，而这个值就是这个容器中允许的最大进程数目。</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># pwd</span>\n/sys/fs/cgroup/pids\n<span class=\"hljs-comment\"># df ./</span>\nFilesystem     1K-blocks  Used Available Use% Mounted on\ncgroup                 <span class=\"hljs-number\">0</span>     <span class=\"hljs-number\">0</span>         <span class=\"hljs-number\">0</span>    - /sys/fs/cgroup/pids\n<span class=\"hljs-comment\"># docker ps</span>\nCONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS               NAMES\n7ecd3aa7fdc1        registry/zombie-proc:v1   <span class=\"hljs-string\">&quot;/app-test 1000&quot;</span>         <span class=\"hljs-number\">37</span> hours ago        Up <span class=\"hljs-number\">37</span> hours                             frosty_yalow\n\n<span class=\"hljs-comment\"># pwd</span>\n/sys/fs/cgroup/pids/system.<span class=\"hljs-built_in\">slice</span>/docker-7ecd3aa7fdc15a1e183813b1899d5d939beafb11833ad6c8b0432536e5b9871c.scope\n\n<span class=\"hljs-comment\"># ls</span>\ncgroup.clone_children  cgroup.procs  notify_on_release  pids.current  pids.events  pids.<span class=\"hljs-built_in\">max</span>  tasks\n<span class=\"hljs-comment\"># echo 1002 &gt; pids.max</span>\n<span class=\"hljs-comment\"># cat pids.max</span>\n<span class=\"hljs-number\">1002</span>\n</code></pre><h3>CPU</h3>\n<blockquote>\n<p>这个cgroup的使用方式就是创建好对应的文件，然后把程序的pid加入进去</p>\n<p>./threads-cpu/threads-cpu 2 &amp;\necho $! &gt; /sys/fs/cgroup/cpu/group2/group3/cgroup.procs</p>\n</blockquote>\n<p>第一个参数是 cpu.cfs_period_us，它是 CFS 算法的一个调度周期，一般它的值是 100000，以 microseconds 为单位，也就 100ms。</p>\n<p>第二个参数是 cpu.cfs_quota_us，它“表示 CFS 算法中，在一个调度周期里这个控制组被允许的运行时间，比如这个值为 50000 时，就是 50ms。</p>\n<p>如果用这个值去除以调度周期（也就是 cpu.cfs_period_us），50ms/100ms = 0.5，这样这个控制组被允许使用的 CPU 最大配额就是 0.5 个 CPU。</p>\n<p>从这里能够看出，cpu.cfs_quota_us 是一个绝对值。如果这个值是 200000，也就是 200ms，那么它除以 period，也就是 200ms/100ms=2。</p>\n<p>第三个参数， cpu.shares。这个值是 CPU Cgroup 对于控制组之间的 CPU 分配比例，它的缺省值是 1024。不同组之间会按照这个值的比例进行分配，例如(group3 中的 cpu.shares 是 1024，而 group4 中的 cpu.shares 是 3072，那么 group3:group4=1:3, 在一台 4 个 CPU 的机器上，当 group3 和 group4 都需要 4 个 CPU 的时候，它们实际分配到的 CPU 分别是这样的：group3 是 1 个，group4 是 3 个。)</p>\n<h2>init进程</h2>\n<p>容器init进程对SIGKILL和SIGTERM信号的处理</p>\n<p>Linux 内核针对每个 Nnamespace 里的 init 进程，把只有 default handler 的信号都给忽略了。</p>\n<p>如果我们自己注册了信号的 handler（应用程序注册信号 handler 被称作&quot;Catch the Signal&quot;），那么这个信号 handler 就不再是 SIG_DFL 。即使是 init 进程在接收到 SIGTERM 之后也是可以退出的。</p>\n<p>不过，由于 SIGKILL 是一个特例，因为 SIGKILL 是不允许被注册用户 handler 的（还有一个不允许注册用户 handler 的信号是 SIGSTOP），那么它只有 SIG_DFL handler。</p>\n<p>所以 init 进程是永远不能被 SIGKILL 所杀，但是可以被 SIGTERM 杀死。</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># ## golang init</span>\n<span class=\"hljs-comment\"># cat /proc/1/status | grep -i SigCgt</span>\n<span class=\"hljs-attr\">SigCgt:</span>     <span class=\"hljs-string\">fffffffe7fc1feff</span>\n\n<span class=\"hljs-comment\"># ## C init</span>\n<span class=\"hljs-comment\"># cat /proc/1/status | grep -i SigCgt</span>\n<span class=\"hljs-attr\">SigCgt:</span>     <span class=\"hljs-number\">0000000000000000</span>\n\n<span class=\"hljs-comment\"># ## bash init</span>\n<span class=\"hljs-comment\"># cat /proc/1/status | grep -i SigCgt</span>\n<span class=\"hljs-attr\">SigCgt:</span>     <span class=\"hljs-number\">0000000000010002</span>\n</code></pre><p>内核为什么必须要给容器中的子进程发 SIGKILL 信号，而不是直接发送 SIGTERM 信号？这样不就不用转发了吗？</p>\n<p>因为 SIGTERM 信号可以被捕获，如果容器内有进程忽略该信号，那么关闭容器后，就有进程残留或僵尸进程。从这点来说，使用 SIGKILL 进行强杀，也是无奈之举。</p>\n<p>容器中一个标准的 init 进程应该具备哪些能力？</p>\n<ul>\n<li>（1）至少可以转发 SIGTERM 信号给容器里的其他进程。</li>\n<li>（2）能够接收外部的 SIGTERM 信号而退出（在 tini 中，init 进程将所有 SIGTERM 信号转发给子进程后，后面还有回收僵尸进程的流程，在这里 init 进程发现所有子进程都退出了，就会让自己也退出）。</li>\n<li>（3）具备清理僵尸进程的能力。</li>\n</ul>\n<h2>如何拿到正确的CPU开销</h2>\n<p>在容器中运行 top 命令，虽然可以看到容器中每个进程的 CPU 使用率，但是 top 中&quot;%Cpu(s)&quot;那一行中显示的数值，并不是这个容器的 CPU 整体使用率，而是容器宿主机的 CPU 使用率。</p>\n<p>对于每个进程，top 都会从 proc 文件系统中每个进程对应的 stat 文件<code>/proc/[pid]/stat</code>中读取 2 个数值utime(进程的用户态部分在 Linux 调度中获得 CPU 的 ticks)和stime(进程的内核态部分在 Linux 调度中获得 CPU 的 ticks)</p>\n<p>由于 /proc/stat 文件是整个节点全局的状态文件，不属于任何一个 Namespace，因此在容器中无法通过读取 /proc/stat 文件来获取单个容器的 CPU 使用率。</p>\n<p>所以要得到单个容器的 CPU 使用率，我们可以从 CPU Cgroup 每个控制组里的统计文件 cpuacct.stat 中获取。单个容器 CPU 使用率 =((utime_2 – utime_1) + (stime_2 – stime_1)) * 100.0 / (HZ * et * 1 )。</p>\n<h1>容器内存</h1>\n<h2>namespace</h2>\n<h2>cgroup</h2>\n<h3>memory</h3>\n<p>Memory Cgroup 的虚拟文件系统的挂载点一般在&quot;/sys/fs/cgroup/memory&quot;这个目录下,可以在 Memory Cgroup 的挂载点目录下，创建一个子目录作为控制组。</p>\n<ul>\n<li>memory.limit_in_bytes: 一个控制组里所有进程可使用内存的最大值</li>\n<li>memory.oom_control: 当控制组中的进程内存使用达到上限值时，这个参数能够决定会不会触发 OOM Killer(不希望触发 OOM Killer，只要执行 echo 1 &gt; memory.oom_control)</li>\n<li>memory.usage_in_bytes: 只读的，它里面的数值是当前控制组里所有进程实际使用的内存总和。</li>\n<li>memory.swappiness: 用于控制是否使用swap分区</li>\n</ul>\n<p>对于每个容器创建后，系统都会为它建立一个 Memory Cgroup 的控制组，容器的所有进程都在这个控制组里。</p>\n<p>容器里所有进程使用的内存量，超过了容器所在 Memory Cgroup 里的内存限制。这时 Linux 系统就会主动杀死容器中的一个进程，往往这会导致整个容器的退出。</p>\n<p>为什么 memory.usage_in_bytes 与 memory.limit_in_bytes 的值只相差了 90KB，我们在容器中还是可以申请出 50MB 的物理内存？</p>\n<p>Memory Cgroup 控制组里 RSS 内存和 Page Cache 内存的和，正好是 memory.usage_in_bytes 的值。</p>\n<p>当控制组里的进程需要申请新的物理内存，而且 memory.usage_in_bytes 里的值超过控制组里的内存上限值 memory.limit_in_bytes，这时我们前面说的 Linux 的内存回收（page frame reclaim）就会被调用起来。</p>\n<p>那么在这个控制组里的 page cache 的内存会根据新申请的内存大小释放一部分，这样我们还是能成功申请到新的物理内存，整个控制组里总的物理内存开销 memory.usage_in_bytes 还是不会超过上限值 memory.limit_in_bytes。</p>\n<pre><code class=\"hljs language-bash\">docker run -d --name mem_alloc registry/<span class=\"hljs-attr\">mem_alloc</span>:v1\n\nsleep <span class=\"hljs-number\">2</span>\n<span class=\"hljs-variable constant_\">CONTAINER_ID</span>=$(sudo docker ps --format <span class=\"hljs-string\">&quot;\\{\\{.ID\\}\\}\\t\\{\\{.Names\\}\\}&quot;</span> | grep -i mem_alloc | awk <span class=\"hljs-string\">&#x27;{print $1}&#x27;</span>)\necho $CONTAINER_ID\n\n<span class=\"hljs-variable constant_\">CGROUP_CONTAINER_PATH</span>=$(find /sys/fs/cgroup/memory/ -name <span class=\"hljs-string\">&quot;*$CONTAINER_ID*&quot;</span>)\necho $CGROUP_CONTAINER_PATH\n\necho <span class=\"hljs-number\">536870912</span> &gt; $CGROUP_CONTAINER_PATH/memory.<span class=\"hljs-property\">limit_in_bytes</span>\ncat $CGROUP_CONTAINER_PATH/memory.<span class=\"hljs-property\">limit_in_bytes</span>\n</code></pre><p>memory.limit_in_bytes: 设置容器最大的内存使用量</p>\n<blockquote>\n<p>如果我们运行docker inspect 命令查看容器退出的原因，就会看到容器处于&quot;exited&quot;状态，并且&quot;OOMKilled&quot;是 true。</p>\n</blockquote>\n<p>在发生 OOM 的时候,Linux根据进程已经使用的物理内存页面数，每个进程的OOM校准值oom_score_adj(在 /proc 文件系统中，每个进程都有一个 /proc//oom_score_adj 的接口文件。我们可以在这个文件中输入 -1000 到 1000 之间的任意一个数值，调整进程被 OOM Kill 的几率。)来决定杀死哪个进程的，用系统总的可用页面数，去乘以 OOM 校准值 oom_score_adj，再加上进程已经使用的物理页面数，计算出来的值越大，那么这个进程被 OOM Kill 的几率也就越大。</p>\n<h2>swap分区</h2>\n<p>memory.swappiness 可以控制这个 Memroy Cgroup 控制组下面匿名内存和 page cache 的回收，取值的范围和工作方式和全局的 swappiness 差不多。这里有一个优先顺序，在 Memory Cgorup 的控制组里，如果你设置了 memory.swappiness 参数，它就会覆盖全局的 swappiness，让全局的 swappiness 在这个控制组里不起作用。</p>\n<p>不过有一点和全局的swappiness不同的是：当 memory.swappiness = 0 的时候，对匿名页的回收是始终禁止的，也就是始终都不会使用 Swap 空间。</p>\n<p>这时 Linux 系统不会再去比较 free 内存和 zone 里的 high water mark 的值，再决定一个 Memory Cgroup 中的匿名内存要不要回收了。</p>\n<h1>容器存储</h1>\n<h2>namespace</h2>\n<h3>mount</h3>\n<p>容器中的根文件系统，其实就是我们做的镜像。</p>\n<p>Mount Namespace 保证了每个容器都有自己独立的文件目录结构。</p>\n<p>可以通过 /proc/mounts 这个路径，找到容器 OverlayFS 对应的 lowerdir 和 upperdir。</p>\n<h2>cgroups</h2>\n<h3>quota</h3>\n<p>从宿主机的角度看，upperdir 就是一个目录，如果容器不断往容器文件系统中写入数据，实际上就是往宿主机的磁盘上写数据，这些数据也就存在于宿主机的磁盘目录中。</p>\n<p>当然对于容器来说，如果有大量的写操作是不建议写入容器文件系统的，一般是需要给容器挂载一个 volume，用来满足大量的文件读写。</p>\n<p>但是不能避免的是，用户在容器中运行的程序有错误，或者进行了错误的配置。</p>\n<p>比如说，我们把 log 写在了容器文件系统上，并且没有做 log rotation，那么时间一久，就会导致宿主机上的磁盘被写满。这样影响的就不止是容器本身了，而是整个宿主机了</p>\n<p>因为 upperdir 在宿主机上也是一个普通的目录, 可以通过限制 upperdir 目录容量的方式，来限制一个容器 OverlayFS 根目录的写入数据量,对于 Linux 上最常用的两个文件系统 XFS 和 ext4，它们有一个特性 Quota,这个特性可以为 Linux 系统里的一个用户（user），一个用户组（group）或者一个项目（project）来限制它们使用文件系统的额度（quota），也就是限制它们可以写入文件系统的文件总量。</p>\n<p>要使用 XFS Quota 特性，必须在文件系统挂载的时候加上对应的 Quota 选项，比如我们目前需要配置 Project Quota，那么这个挂载参数就是&quot;pquota&quot;。对于根目录来说，这个参数必须作为一个内核启动的参数&quot;rootflags=pquota&quot;，这样设置就可以保证根目录在启动挂载的时候，带上 XFS Quota 的特性并且支持 Project 模式。我们可以从 /proc/mounts 信息里，看看根目录是不是带&quot;prjquota&quot;字段。如果里面有这个字段，就可以确保文件系统已经带上了支持 project 模式的 XFS quota 特性。</p>\n<p>通过 xfs_quota 这条命令,给一个指定的目录打上一个 Project ID, 未这个id做Quota限制</p>\n<p><code>xfs_quota -x -c &#39;limit -p bhard=10m 101&#39; /</code></p>\n<p>在用 docker run 启动容器的时候，加上一个参数 --storage-opt size= ，就能限制住容器 OverlayFS 文件系统可写入的最大数据量了。</p>\n<h3>blkio</h3>\n<blockquote>\n<p>Direct I/O可以通过blkio Cgroup来限制磁盘I/O，但是Buffered I/O不能被限制。这个问题只有在cgroupv2版本中才解决了</p>\n</blockquote>\n<p>不过容器文件系统并不适合频繁地读写。对于频繁读写的数据，容器需要把他们到放到&quot;volume&quot;中。这里的volume可以是一个本地的磁盘，也可以是一个网络磁盘。如果多个容器同时读写是会产生影响的，我们可以限制某一个的读写速率</p>\n<p>Cgroup v1中有blkio子系统, 可以来限制磁盘的I/O。不过blkio子系统对于磁盘I/O的限制，并不像CPU，Memory那么直接</p>\n<p>在Cgroups v1里，blkio Cgroup的虚拟文件系统挂载点一般在&quot;/sys/fs/cgroup/blkio/&quot;。</p>\n<ul>\n<li>blkio.throttle.read_iops_device</li>\n<li>blkio.throttle.read_bps_device</li>\n<li>blkio.throttle.write_iops_device</li>\n<li>blkio.throttle.write_bps_device</li>\n</ul>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># &quot;252:16&quot;是要写入的设备例如/dev/vdb的主次设备号，你可以通过 ls -l /dev/vdb 看到这两个值</span>\n<span class=\"hljs-comment\"># 设置写入吞吐量不超过10MB/s</span>\necho <span class=\"hljs-string\">&quot;252:16 10485760&quot;</span> &gt; $CGROUP_CONTAINER_PATH/blkio.throttle.write_bps_device\n</code></pre><h3>io</h3>\n<blockquote>\n<p>cgroupv2 版本中的系统，用于解决无法对buffered io限速的问题</p>\n</blockquote>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># Create a new control group</span>\nmkdir -p /sys/fs/cgroup/unified/iotest\n\n<span class=\"hljs-comment\"># enable the io and memory controller subsystem</span>\necho <span class=\"hljs-string\">&quot;+io +memory&quot;</span> &gt; /sys/fs/cgroup/unified/cgroup.subtree_control\n\n<span class=\"hljs-comment\"># Add current bash pid in iotest control group.</span>\n<span class=\"hljs-comment\"># Then all child processes of the bash will be in iotest group too,</span>\n<span class=\"hljs-comment\"># including the fio</span>\n\necho $$ &gt;/sys/fs/cgroup/unified/iotest/cgroup.procs\n\n<span class=\"hljs-comment\"># 256:16 are device major and minor ids, /mnt is on the device.</span>\necho <span class=\"hljs-string\">&quot;252:16 wbps=10485760&quot;</span> &gt; /sys/fs/cgroup/unified/iotest/io.<span class=\"hljs-built_in\">max</span>\ncd /mnt\n<span class=\"hljs-comment\">#Run the fio in non direct I/O mode</span>\nfio -iodepth=<span class=\"hljs-number\">1</span> -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=<span class=\"hljs-number\">1</span>  -name=./fio.test\n</code></pre><h2>容器镜像的文件系统</h2>\n<p>在容器里，运行 df 命令，你可以看到在容器中根目录 (/) 的文件系统类型是&quot;overlay&quot;，它不是我们在普通 Linux 节点上看到的 Ext4 或者 XFS 之类常见的文件系统。</p>\n<p>如果没有特别的容器文件系统，只是普通的 Ext4 或者 XFS 文件系统，那么每次启动一个容器，就需要把一个镜像文件下载并且存储在宿主机上。overlay文件系统就是用来解决这种冗余的</p>\n<blockquote>\n<p>在 Linux 内核 3.18 版本中，OverlayFS 代码正式合入 Linux 内核的主分支。在这之后，OverlayFS 也就逐渐成为各个主流 Linux 发行版本里缺省使用的容器文件系统了。</p>\n</blockquote>\n<p>overlay在分类中属于UnionFS，UnionFS 这类文件系统实现的主要功能是把多个目录（处于不同的分区）一起挂载（mount）在一个目录下。</p>\n<img src=\"/images/blogs/unionfs示意图.jpeg\" />\n\n<blockquote>\n<p>&quot;work/&quot;，一个存放临时文件的目录，OverlayFS 中如果有文件修改，就会在中间过程中临时存放文件到这里。</p>\n</blockquote>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># !/bin/bash</span>\n\numount ./merged\nrm upper lower merged work -r\n\nmkdir upper lower merged work\necho <span class=\"hljs-string\">&quot;I&#x27;m from lower!&quot;</span> &gt; lower/in_lower.txt\necho <span class=\"hljs-string\">&quot;I&#x27;m from upper!&quot;</span> &gt; upper/in_upper.txt\n<span class=\"hljs-comment\"># `in_both` is in both directories</span>\necho <span class=\"hljs-string\">&quot;I&#x27;m from lower!&quot;</span> &gt; lower/in_both.txt\necho <span class=\"hljs-string\">&quot;I&#x27;m from upper!&quot;</span> &gt; upper/in_both.txt\n\nsudo mount -t overlay overlay \\\n -o lowerdir=./lower,upperdir=./upper,workdir=./work \\\n ./merged\n</code></pre><p>OverlayFS 也是把多个目录合并挂载，被挂载的目录分为两大类：lowerdir 和 upperdir。</p>\n<p>lowerdir 允许有多个目录，在被挂载后，这些目录里的文件都是不会被修改或者删除的，也就是只读的；upperdir 只有一个，不过这个目录是可读写的，挂载点目录中的所有文件修改都会在 upperdir 中反映出来。</p>\n<p>在merged文件夹中进行操作的效果:</p>\n<ul>\n<li>新建文件，这个文件会出现在 upper/ 目录中。</li>\n<li>删除文件，如果我们删除&quot;in_upper.txt&quot;，那么这个文件会在 upper/ 目录中消失。如果删除&quot;in_lower.txt&quot;, 在 lower/ 目录里的&quot;in_lower.txt&quot;文件不会有变化，只是在 upper/ 目录中增加了一个特殊文件来告诉 OverlayFS，&quot;in_lower.txt&#39;这个文件不能出现在 merged/ 里了，这就表示它已经被删除了。</li>\n<li>修改文件, 类似如果修改&quot;in_lower.txt&quot;，那么就会在 upper/ 目录中新建一个&quot;in_lower.txt&quot;文件，包含更新的内容，而在 lower/ 中的原来的实际文件&quot;in_lower.txt&quot;不会改变。</li>\n</ul>\n<h2>磁盘读写</h2>\n<p>Blkio Cgroup: 在容器中对磁盘I/O的限速</p>\n<p>Linux内存和文件系统管理对容器IO的影响: Page Frame Reclaim, Dirty Page</p>\n<h1>容器网络</h1>\n<h2>namespace</h2>\n<h3>network</h3>\n<p>隔离的资源包括</p>\n<ul>\n<li>第一种，网络设备，lo，eth0等</li>\n<li>第二种是 IPv4 和 IPv6 协议栈。IP 层以及上面的 TCP 和 UDP 协议栈也是每个 Namespace 独立工作的。所以 IP、TCP、UDP 的很多协议，它们的相关参数也是每个 Namespace 独立的，这些参数大多数都在 /proc/sys/net/ 目录下面，同时也包括了 TCP 和 UDP 的 port 资源。</li>\n<li>第三种，IP 路由表</li>\n<li>第四种是iptables 规则了</li>\n<li>第五种是网络的状态信息，这些信息你可以从 /proc/net 和 /sys/class/net 里得到</li>\n</ul>\n<p>tcp_keepalive 的三个参数都是重新初始化的，而 tcp_congestion_control 的值是从 Host Namespace 里复制过来的。</p>\n<p>在启动普通容器，尝试一下在容器里去修改&quot;/proc/sys/net/&quot;下的参数，会报错</p>\n<blockquote>\n<p>只读模式下，容器中&quot;/proc/sys/&quot;是只读 mount 的，那么在容器里是不能修改&quot;/proc/sys/net/&quot;下面的任何参数了。</p>\n</blockquote>\n<p>想修改 Network Namespace 里的网络参数，要选择容器刚刚启动，而容器中的应用程序还没启动之前进行。runC 也在对 /proc/sys 目录做 read-only mount 之前，预留出了修改接口，就是用来修改容器里 &quot;/proc/sys&quot;下参数的，同样也是 sysctl 的参数。而 Docker 的–sysctl或者 Kubernetes 里的allowed-unsafe-sysctls特性也都利用了 runC 的 sysctl 参数修改接口，允许容器在启动时修改容器 Namespace 里的参数。</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># docker run -d --name net_para --sysctl net.ipv4.tcp_keepalive_time=600 centos:8.1.1911 sleep 3600</span>\n7efed88a44d64400ff5a6d38fdcc73f2a74a7bdc3dbc7161060f2f7d0be170d1\n<span class=\"hljs-comment\"># docker exec net_para cat /proc/sys/net/ipv4/tcp_keepalive_time</span>\n<span class=\"hljs-number\">600</span>\n</code></pre><img src=\"/images/blogs/networknamespace工具包.jpeg\" />\n\n<h2>cgroup</h2>\n<h2>容器网络接口</h2>\n<img src=\"/images/blogs/network网络模式.jpeg\" />\n\n<p>第一步，就是要让数据包从容器的 Network Namespace 发送到 Host Network Namespace 上。</p>\n<p>一般来说就只有两类设备接口：一类是veth，另外一类是 macvlan/ipvlan。</p>\n<p>用 Docker 启动的容器缺省的网络接口用的也是这个 veth</p>\n<img src=\"/images/blogs/veth.jpeg\" />\n\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># 示例 创建一个无网络的容器，手动创建一个veth网卡</span>\ndocker run -d --name <span class=\"hljs-keyword\">if</span>-test --network none centos:<span class=\"hljs-number\">8.1</span><span class=\"hljs-number\">.1911</span> sleep <span class=\"hljs-number\">36000</span>\n\npid=$(ps -ef | grep <span class=\"hljs-string\">&quot;sleep 36000&quot;</span> | grep -v grep | awk <span class=\"hljs-string\">&#x27;{print $2}&#x27;</span>)\necho $pid\n\n<span class=\"hljs-comment\"># 在&quot;/var/run/netns/&quot;的目录下建立一个符号链接，指向这个容器的 Network Namespace。完成这步操作之后，在后面的&quot;ip netns&quot;操作里，就可以用 pid 的值作为这个容器的 Network Namesapce 的标识了。</span>\nln -s /proc/$pid/ns/net /var/run/netns/$pid\n\n<span class=\"hljs-comment\"># 建立一对 veth 的虚拟设备接口</span>\nip link add name veth_host <span class=\"hljs-built_in\">type</span> veth peer name veth_container\n<span class=\"hljs-comment\"># 把 veth_container 这个接口放入到容器的 Network Namespace 中</span>\nip link <span class=\"hljs-built_in\">set</span> veth_container netns $pid\n\n<span class=\"hljs-comment\"># In the container, setup veth_container</span>\nip netns <span class=\"hljs-built_in\">exec</span> $pid ip link <span class=\"hljs-built_in\">set</span> veth_container name eth0\nip netns <span class=\"hljs-built_in\">exec</span> $pid ip addr add <span class=\"hljs-number\">172.17</span><span class=\"hljs-number\">.1</span><span class=\"hljs-number\">.2</span>/<span class=\"hljs-number\">16</span> dev eth0\nip netns <span class=\"hljs-built_in\">exec</span> $pid ip link <span class=\"hljs-built_in\">set</span> eth0 up\nip netns <span class=\"hljs-built_in\">exec</span> $pid ip route add default via <span class=\"hljs-number\">172.17</span><span class=\"hljs-number\">.0</span><span class=\"hljs-number\">.1</span>\n\n<span class=\"hljs-comment\"># In the host, set veth_host up</span>\nip link <span class=\"hljs-built_in\">set</span> veth_host up\n</code></pre><p>ipvlan网络配置方式</p>\n<blockquote>\n<p>对于延时敏感的应用程序，我们可以考虑使用 ipvlan/macvlan 网络接口的容器。不过，由于 ipvlan/macvlan 网络接口直接挂载在物理网络接口上，对于需要使用 iptables 规则的容器，比如 Kubernetes 里使用 service 的容器，就不能工作了。</p>\n</blockquote>\n<p>无论是 macvlan 还是 ipvlan，它们都是在一个物理的网络接口上再配置几个虚拟的网络接口。在这些虚拟的网络接口上，都可以配置独立的 IP，并且这些 IP 可以属于不同的 Namespace。对于 macvlan，每个虚拟网络接口都有自己独立的 mac 地址；而 ipvlan 的虚拟网络接口是和物理网络接口共享同一个 mac 地址。而且它们都有自己的 L2/L3 的配置方式</p>\n<img src=\"/images/blogs/ipvlan.jpeg\" />\n\n<pre><code class=\"hljs language-bash\">docker run --init --name lat-test-<span class=\"hljs-number\">1</span> --network none -d registry/latency-test:v1 sleep <span class=\"hljs-number\">36000</span>\n\npid1=$(docker inspect lat-test-<span class=\"hljs-number\">1</span> | grep -i Pid | head -n <span class=\"hljs-number\">1</span> | awk <span class=\"hljs-string\">&#x27;{print $2}&#x27;</span> | awk -F <span class=\"hljs-string\">&quot;,&quot;</span> <span class=\"hljs-string\">&#x27;{print $1}&#x27;</span>)\necho $pid1\nln -s /proc/$pid1/ns/net /var/run/netns/$pid1\n \nip link add link eth0 ipvt1 <span class=\"hljs-built_in\">type</span> ipvlan mode l2\nip link <span class=\"hljs-built_in\">set</span> dev ipvt1 netns $pid1\n\nip netns <span class=\"hljs-built_in\">exec</span> $pid1 ip link <span class=\"hljs-built_in\">set</span> ipvt1 name eth0\nip netns <span class=\"hljs-built_in\">exec</span> $pid1 ip addr add <span class=\"hljs-number\">172.17</span><span class=\"hljs-number\">.3</span><span class=\"hljs-number\">.2</span>/<span class=\"hljs-number\">16</span> dev eth0\nip netns <span class=\"hljs-built_in\">exec</span> $pid1 ip link <span class=\"hljs-built_in\">set</span> eth0 up\n</code></pre><p>第二步，数据包发到了 Host Network Namespace 之后，还要解决数据包怎么从宿主机上的 eth0 发送出去的问题。</p>\n<p>这一步呢，就是一个普通 Linux 节点上数据包转发的问题了。用 nat 来做个转发，或者建立 Overlay 网络发送，也可以通过配置 proxy arp 加路由的方法来实现。</p>\n<p>Docker 缺省使用的是 bridge + nat 的转发方式</p>\n<p>Docker 程序在节点上安装完之后，就会自动建立了一个 docker0 的 bridge interface。所以我们只需要把第一步中建立的 veth_host 这个设备，接入到 docker0 这个 bridge 上。</p>\n<img src=\"/images/blogs/网络第二步.png\" />\n\n<p>容器和 docker0 组成了一个子网，docker0 上的 IP 就是这个子网的网关 IP。如果我们要让子网通过宿主机上 eth0 去访问外网的话，那么加上 iptables 的规则就可以了<code>iptables -P FORWARD ACCEPT</code></p>\n<p>两个网络设备接口之间的数据包转发，需要ip_forward参数，所以还需要把这个改为1</p>\n<pre><code class=\"hljs language-bash\">echo <span class=\"hljs-number\">1</span> &gt; <span class=\"hljs-regexp\">/proc/</span>sys/net/ipv4/ip_forward\n</code></pre><h2>容器网络的问题</h2>\n<p>延时增加, 因为多了虚拟网卡的转发以及各种路由</p>\n<p>乱序包的增加</p>\n<p>在云平台的这种网络环境里，网络包乱序 +SACK 之后，产生的数据包重传的量要远远高于网络丢包引起的重传。</p>\n<p>把 RPS 的这个特性配置到 veth 网络接口上，来减少数据包乱序的几率。不过RPS 的配置还是会带来额外的系统开销，在某些网络环境中会引起 softirq CPU 使用率的增大</p>\n<p>尽管容器中 root 用户的 Linux capabilities 已经减少了很多，但是在没有 User Namespace 的情况下，容器中 root 用户和宿主机上的 root 用户的 uid 是完全相同的，一旦有软件的漏洞，容器中的 root 用户就可以操控整个宿主机。</p>\n<p>User Namespace，它带来的好处有两个。一个是把容器中 root 用户（uid 0）映射成宿主机上的普通用户，另外一个好处是在云平台里对于容器 uid 的分配要容易些。</p>\n<p>为了减少安全风险，业界都是建议在容器中以非 root 用户来运行进程。不过在没有 User Namespace 的情况下，在容器中使用非 root 用户，对于容器云平台来说，对 uid 的管理会比较麻烦。</p>\n<p>除了在容器中以非 root 用户来运行进程外，Docker 和 podman 都支持了 rootless container，也就是说它们都可以以非 root 用户来启动和管理容器，这样就进一步降低了容器的安全风险。</p>\n<h1>容器安全</h1>\n<p>对于容器的 root 用户，缺省只赋予了 15 个 capabilities。如果我们发现容器中进程的权限不够，就需要分析它需要的最小 capabilities 集合，而不是直接赋予容器&quot;privileged&quot;。</p>\n<p>因为&quot;privileged&quot;包含了所有的 Linux capabilities, 这样&quot;privileged&quot;就可以轻易获取宿主机上的所有资源，这会对宿主机的安全产生威胁。容器平台上是基本不允许把容器直接设置为&quot;privileged&quot;的，我们需要根据容器中进程需要的最少特权来赋予 capabilities。</p>\n<p>假设容器里需要使用 iptables。因为使用 iptables 命令，只需要设置 CAP_NET_ADMIN 这个 capability 就行。那么我们只要在运行 Docker 的时候，给这个容器再多加一个 NET_ADMIN 参数就可以了。</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># docker run --name iptables --cap-add NET_ADMIN -it registry/iptables:v1 bash</span>\n[root@cfedf124dcf1 /]<span class=\"hljs-comment\"># iptables -L</span>\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination\n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\n</code></pre><h2>not a root</h2>\n<p>如果不想让容器以 root 用户运行，最直接的办法就是给容器指定一个普通用户 uid</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-string\">docker</span> <span class=\"hljs-string\">run</span> <span class=\"hljs-string\">-ti</span> <span class=\"hljs-string\">--name</span> <span class=\"hljs-string\">root_example</span> <span class=\"hljs-string\">-u</span> <span class=\"hljs-number\">6667</span><span class=\"hljs-string\">:6667</span> <span class=\"hljs-string\">-v</span> <span class=\"hljs-string\">/etc:/mnt</span>  <span class=\"hljs-string\">centos</span> <span class=\"hljs-string\">bash</span>\n</code></pre><p>还有另外一个办法，就是我们在创建容器镜像的时候，用 Dockerfile 为容器镜像里建立一个用户。这样操作以后，容器里缺省的进程都会以这个用户启动。</p>\n<pre><code class=\"hljs language-dockerfile\"><span class=\"hljs-string\">FROM</span> <span class=\"hljs-string\">centos</span>\n\n<span class=\"hljs-string\">RUN</span> <span class=\"hljs-string\">adduser</span> <span class=\"hljs-string\">-u</span> <span class=\"hljs-number\">6667 </span><span class=\"hljs-string\">nonroot</span>\n<span class=\"hljs-string\">USER</span> <span class=\"hljs-string\">nonroot</span>\n</code></pre><p>但是这种方法会导致当多个客户的容器在同一个节点上运行的时候，其实就都使用了宿主机上 uid 6667。然而Linux 系统上，每个用户下的资源是有限制的，比如打开文件数目（open files）、最大进程数目（max user processes）等等。一旦有很多个容器共享一个 uid，这些容器就很可能很快消耗掉这个 uid 下的资源，这样很容易导致这些容器都不能再正常工作。</p>\n<h2>Usernamespace</h2>\n<blockquote>\n<p>k8s还未引入</p>\n</blockquote>\n<p>User Namespace 隔离了一台 Linux 节点上的 User ID（uid）和 Group ID（gid），它给 Namespace 中的 uid/gid 的值与宿主机上的 uid/gid 值建立了一个映射关系。经过 User Namespace 的隔离，我们在 Namespace 中看到的进程的 uid/gid，就和宿主机 Namespace 中看到的 uid 和 gid 不一样了。</p>\n<blockquote>\n<p>跟 Docker 相比，podman 不再有守护进程 dockerd，而是直接通过 fork/execve 的方式来启动一个新的容器。这种方式启动容器更加简单，也更容易维护。</p>\n</blockquote>\n<p><code>podman run -ti  -v /etc:/mnt --uidmap 0:2000:1000 centos bash</code></p>\n<p><code>0:2000:1000</code>意思是第一个 0 是指在新的 Namespace 里 uid 从 0 开始，中间的那个 2000 指的是 Host Namespace 里被映射的 uid 从 2000 开始，最后一个 1000 是指总共需要连续映射 1000 个 uid。</p>\n<p>容器里的uid 0 是被映射到宿主机上的 uid 2000</p>\n<h2>rootless container</h2>\n<p>rootless container 中的&quot;rootless&quot;不仅仅指容器中以非 root 用户来运行进程，还指以非 root 用户来创建容器，管理容器。也就是说，启动容器的时候，Docker 或者 podman 是以非 root 用户来执行的。</p>\n<pre><code class=\"hljs language-bash\">$ <span class=\"hljs-built_in\">id</span>\nuid=<span class=\"hljs-number\">1001</span>(redhat) gid=<span class=\"hljs-number\">1001</span>(redhat) groups=<span class=\"hljs-number\">1001</span>(redhat)\n$ podman run -it  ubi7/ubi bash   <span class=\"hljs-comment\">### 在宿主机上以redhat用户启动容器</span>\n[root@206f6d5cb033 /]<span class=\"hljs-comment\"># id     ### 容器中的用户是root</span>\nuid=<span class=\"hljs-number\">0</span>(root) gid=<span class=\"hljs-number\">0</span>(root) groups=<span class=\"hljs-number\">0</span>(root)\n[root@206f6d5cb033 /]<span class=\"hljs-comment\"># sleep 3600   ### 在容器中启动一个sleep 进程</span>\n\n<span class=\"hljs-comment\"># ps -ef |grep sleep   ###在宿主机上查看容器sleep进程对应的用户</span>\nredhat   <span class=\"hljs-number\">29433</span> <span class=\"hljs-number\">29410</span>  <span class=\"hljs-number\">0</span> 05:<span class=\"hljs-number\">14</span> pts/<span class=\"hljs-number\">0</span>    <span class=\"hljs-number\">00</span>:<span class=\"hljs-number\">00</span>:<span class=\"hljs-number\">00</span> sleep <span class=\"hljs-number\">3600</span>\n</code></pre>","frontMatter":{"readingTime":{"text":"46 min read","minutes":45.75,"time":2745000,"words":9150},"slug":"容器核心点概览","fileName":"容器核心点概览.md","title":"容器核心点","date":"2022-08-14T00:00:00.000Z","tags":["容器"],"draft":false,"summary":"容器使用与主机的区别"}},"prev":{"title":"grpc手册","date":"2022-08-13T00:00:00.000Z","tags":["中间件"],"draft":false,"summary":"grpc介绍以及使用","slug":"grpc手册"},"next":{"title":"ssh环境配置","date":"2022-10-08T00:00:00.000Z","tags":["实践"],"draft":false,"summary":"ssh环境配置","slug":"ssh环境配置"}},"__N_SSG":true}